param_path: param.yaml
num_of_training_epochs: 2000
num_of_input_nodes: 1
num_of_prediction_epochs: 100
learning_rate: 0.1
train_data_path: ../train_data/normal.npy
optimizer: GradientDescentOptimizer
num_of_output_nodes: 1
forget_bias: 1.0
length_of_sequences: 50
size_of_mini_batch: 50
seed: 0
num_of_hidden_nodes: 2
train_data: [[  0.00000000e+00   1.25333234e-01]
 [  1.25333234e-01   2.48689887e-01]
 [  2.48689887e-01   3.68124553e-01]
 ..., 
 [ -3.68124553e-01  -2.48689887e-01]
 [ -2.48689887e-01  -1.25333234e-01]
 [ -1.25333234e-01   3.92877345e-15]]
train#10, train loss: 4.488009e-01
train#20, train loss: 4.073704e-01
train#30, train loss: 2.701567e-01
train#40, train loss: 1.409866e-01
train#50, train loss: 1.082554e-01
train#60, train loss: 9.621155e-02
train#70, train loss: 7.830937e-02
train#80, train loss: 6.070483e-02
train#90, train loss: 5.979984e-02
train#100, train loss: 4.002890e-02
train#110, train loss: 4.525840e-02
train#120, train loss: 3.551317e-02
train#130, train loss: 3.256310e-02
train#140, train loss: 3.066458e-02
train#150, train loss: 2.351280e-02
train#160, train loss: 2.513366e-02
train#170, train loss: 1.982205e-02
train#180, train loss: 1.737243e-02
train#190, train loss: 1.480669e-02
train#200, train loss: 1.326685e-02
train#210, train loss: 1.130697e-02
train#220, train loss: 1.251987e-02
train#230, train loss: 8.099410e-03
train#240, train loss: 8.376968e-03
train#250, train loss: 8.212639e-03
train#260, train loss: 5.931962e-03
train#270, train loss: 5.977654e-03
train#280, train loss: 6.137682e-03
train#290, train loss: 4.624938e-03
train#300, train loss: 5.293766e-03
train#310, train loss: 3.821040e-03
train#320, train loss: 3.912759e-03
train#330, train loss: 5.656627e-03
train#340, train loss: 4.687992e-03
train#350, train loss: 3.135480e-03
train#360, train loss: 4.760012e-03
train#370, train loss: 4.606024e-03
train#380, train loss: 4.828892e-03
train#390, train loss: 3.246640e-03
train#400, train loss: 3.487277e-03
train#410, train loss: 3.798494e-03
train#420, train loss: 3.737300e-03
train#430, train loss: 3.283018e-03
train#440, train loss: 3.833024e-03
train#450, train loss: 3.450718e-03
train#460, train loss: 2.830018e-03
train#470, train loss: 2.533427e-03
train#480, train loss: 3.615558e-03
train#490, train loss: 3.048615e-03
train#500, train loss: 3.669769e-03
train#510, train loss: 2.953699e-03
train#520, train loss: 3.873735e-03
train#530, train loss: 2.334525e-03
train#540, train loss: 3.529924e-03
train#550, train loss: 1.915821e-03
train#560, train loss: 3.754761e-03
train#570, train loss: 3.224552e-03
train#580, train loss: 2.909251e-03
train#590, train loss: 3.037831e-03
train#600, train loss: 3.251110e-03
train#610, train loss: 2.348449e-03
train#620, train loss: 2.818667e-03
train#630, train loss: 2.195684e-03
train#640, train loss: 2.713495e-03
train#650, train loss: 2.819176e-03
train#660, train loss: 2.735571e-03
train#670, train loss: 2.299146e-03
train#680, train loss: 2.791684e-03
train#690, train loss: 2.428086e-03
train#700, train loss: 1.739636e-03
train#710, train loss: 2.377989e-03
train#720, train loss: 2.375856e-03
train#730, train loss: 2.479205e-03
train#740, train loss: 2.355516e-03
train#750, train loss: 1.829221e-03
train#760, train loss: 2.971299e-03
train#770, train loss: 2.084609e-03
train#780, train loss: 2.449958e-03
train#790, train loss: 2.451132e-03
train#800, train loss: 2.229714e-03
train#810, train loss: 2.356714e-03
train#820, train loss: 2.923964e-03
train#830, train loss: 2.276304e-03
train#840, train loss: 1.898908e-03
train#850, train loss: 2.354089e-03
train#860, train loss: 2.321817e-03
train#870, train loss: 2.493344e-03
train#880, train loss: 2.556324e-03
train#890, train loss: 2.472818e-03
train#900, train loss: 2.325682e-03
train#910, train loss: 2.117123e-03
train#920, train loss: 1.629859e-03
train#930, train loss: 2.081266e-03
train#940, train loss: 2.263014e-03
train#950, train loss: 1.859708e-03
train#960, train loss: 2.322174e-03
train#970, train loss: 1.640191e-03
train#980, train loss: 2.020457e-03
train#990, train loss: 2.100250e-03
train#1000, train loss: 1.651491e-03
train#1010, train loss: 1.918354e-03
train#1020, train loss: 1.624082e-03
train#1030, train loss: 1.516747e-03
train#1040, train loss: 1.794732e-03
train#1050, train loss: 1.499988e-03
train#1060, train loss: 1.654607e-03
train#1070, train loss: 2.033888e-03
train#1080, train loss: 1.933176e-03
train#1090, train loss: 1.907101e-03
train#1100, train loss: 2.196047e-03
train#1110, train loss: 1.504706e-03
train#1120, train loss: 1.956201e-03
train#1130, train loss: 1.865868e-03
train#1140, train loss: 1.350025e-03
train#1150, train loss: 2.374910e-03
train#1160, train loss: 1.654250e-03
train#1170, train loss: 1.853280e-03
train#1180, train loss: 1.940512e-03
train#1190, train loss: 1.389499e-03
train#1200, train loss: 1.415395e-03
train#1210, train loss: 1.450176e-03
train#1220, train loss: 1.493976e-03
train#1230, train loss: 1.948371e-03
train#1240, train loss: 1.450249e-03
train#1250, train loss: 1.776731e-03
train#1260, train loss: 1.584008e-03
train#1270, train loss: 1.478674e-03
train#1280, train loss: 1.803320e-03
train#1290, train loss: 1.387364e-03
train#1300, train loss: 1.220425e-03
train#1310, train loss: 1.288279e-03
train#1320, train loss: 1.313112e-03
train#1330, train loss: 1.264082e-03
train#1340, train loss: 1.213191e-03
train#1350, train loss: 1.520619e-03
train#1360, train loss: 1.188657e-03
train#1370, train loss: 1.389712e-03
train#1380, train loss: 1.333578e-03
train#1390, train loss: 1.295663e-03
train#1400, train loss: 1.629750e-03
train#1410, train loss: 1.221517e-03
train#1420, train loss: 1.222682e-03
train#1430, train loss: 1.177906e-03
train#1440, train loss: 1.241496e-03
train#1450, train loss: 1.446407e-03
train#1460, train loss: 1.029894e-03
train#1470, train loss: 1.111228e-03
train#1480, train loss: 1.235554e-03
train#1490, train loss: 1.150397e-03
train#1500, train loss: 1.235189e-03
train#1510, train loss: 1.233028e-03
train#1520, train loss: 1.180855e-03
train#1530, train loss: 1.475710e-03
train#1540, train loss: 1.173846e-03
train#1550, train loss: 9.566372e-04
train#1560, train loss: 1.186126e-03
train#1570, train loss: 1.244487e-03
train#1580, train loss: 1.105938e-03
train#1590, train loss: 9.957292e-04
train#1600, train loss: 9.269919e-04
train#1610, train loss: 1.383703e-03
train#1620, train loss: 1.258167e-03
train#1630, train loss: 1.020429e-03
train#1640, train loss: 1.489863e-03
train#1650, train loss: 8.836834e-04
train#1660, train loss: 1.184768e-03
train#1670, train loss: 9.613810e-04
train#1680, train loss: 8.785140e-04
train#1690, train loss: 9.381947e-04
train#1700, train loss: 1.341505e-03
train#1710, train loss: 8.105508e-04
train#1720, train loss: 8.920159e-04
train#1730, train loss: 1.164646e-03
train#1740, train loss: 1.049635e-03
train#1750, train loss: 1.080170e-03
train#1760, train loss: 1.046513e-03
train#1770, train loss: 1.026049e-03
train#1780, train loss: 1.292142e-03
train#1790, train loss: 8.281851e-04
train#1800, train loss: 1.042676e-03
train#1810, train loss: 6.788527e-04
train#1820, train loss: 9.042205e-04
train#1830, train loss: 9.649323e-04
train#1840, train loss: 1.082720e-03
train#1850, train loss: 9.794760e-04
train#1860, train loss: 1.056432e-03
train#1870, train loss: 1.192114e-03
train#1880, train loss: 1.059685e-03
train#1890, train loss: 1.046425e-03
train#1900, train loss: 1.122529e-03
train#1910, train loss: 8.395253e-04
train#1920, train loss: 6.822581e-04
train#1930, train loss: 1.074477e-03
train#1940, train loss: 8.965435e-04
train#1950, train loss: 9.663589e-04
train#1960, train loss: 7.781641e-04
train#1970, train loss: 9.239154e-04
train#1980, train loss: 1.039140e-03
train#1990, train loss: 6.953090e-04
train#2000, train loss: 8.062712e-04
losses: [[  1.00000000e+01   4.48800892e-01]
 [  2.00000000e+01   4.07370418e-01]
 [  3.00000000e+01   2.70156711e-01]
 [  4.00000000e+01   1.40986606e-01]
 [  5.00000000e+01   1.08255446e-01]
 [  6.00000000e+01   9.62115452e-02]
 [  7.00000000e+01   7.83093721e-02]
 [  8.00000000e+01   6.07048273e-02]
 [  9.00000000e+01   5.97998351e-02]
 [  1.00000000e+02   4.00288962e-02]
 [  1.10000000e+02   4.52583991e-02]
 [  1.20000000e+02   3.55131663e-02]
 [  1.30000000e+02   3.25631015e-02]
 [  1.40000000e+02   3.06645799e-02]
 [  1.50000000e+02   2.35128030e-02]
 [  1.60000000e+02   2.51336619e-02]
 [  1.70000000e+02   1.98220480e-02]
 [  1.80000000e+02   1.73724350e-02]
 [  1.90000000e+02   1.48066906e-02]
 [  2.00000000e+02   1.32668493e-02]
 [  2.10000000e+02   1.13069713e-02]
 [  2.20000000e+02   1.25198746e-02]
 [  2.30000000e+02   8.09940975e-03]
 [  2.40000000e+02   8.37696809e-03]
 [  2.50000000e+02   8.21263902e-03]
 [  2.60000000e+02   5.93196228e-03]
 [  2.70000000e+02   5.97765436e-03]
 [  2.80000000e+02   6.13768212e-03]
 [  2.90000000e+02   4.62493813e-03]
 [  3.00000000e+02   5.29376604e-03]
 [  3.10000000e+02   3.82103981e-03]
 [  3.20000000e+02   3.91275855e-03]
 [  3.30000000e+02   5.65662747e-03]
 [  3.40000000e+02   4.68799192e-03]
 [  3.50000000e+02   3.13547999e-03]
 [  3.60000000e+02   4.76001250e-03]
 [  3.70000000e+02   4.60602390e-03]
 [  3.80000000e+02   4.82889218e-03]
 [  3.90000000e+02   3.24663986e-03]
 [  4.00000000e+02   3.48727684e-03]
 [  4.10000000e+02   3.79849365e-03]
 [  4.20000000e+02   3.73729994e-03]
 [  4.30000000e+02   3.28301825e-03]
 [  4.40000000e+02   3.83302383e-03]
 [  4.50000000e+02   3.45071754e-03]
 [  4.60000000e+02   2.83001782e-03]
 [  4.70000000e+02   2.53342744e-03]
 [  4.80000000e+02   3.61555791e-03]
 [  4.90000000e+02   3.04861460e-03]
 [  5.00000000e+02   3.66976880e-03]
 [  5.10000000e+02   2.95369863e-03]
 [  5.20000000e+02   3.87373543e-03]
 [  5.30000000e+02   2.33452464e-03]
 [  5.40000000e+02   3.52992443e-03]
 [  5.50000000e+02   1.91582146e-03]
 [  5.60000000e+02   3.75476130e-03]
 [  5.70000000e+02   3.22455191e-03]
 [  5.80000000e+02   2.90925056e-03]
 [  5.90000000e+02   3.03783081e-03]
 [  6.00000000e+02   3.25110951e-03]
 [  6.10000000e+02   2.34844862e-03]
 [  6.20000000e+02   2.81866710e-03]
 [  6.30000000e+02   2.19568354e-03]
 [  6.40000000e+02   2.71349540e-03]
 [  6.50000000e+02   2.81917630e-03]
 [  6.60000000e+02   2.73557147e-03]
 [  6.70000000e+02   2.29914626e-03]
 [  6.80000000e+02   2.79168435e-03]
 [  6.90000000e+02   2.42808554e-03]
 [  7.00000000e+02   1.73963618e-03]
 [  7.10000000e+02   2.37798924e-03]
 [  7.20000000e+02   2.37585604e-03]
 [  7.30000000e+02   2.47920537e-03]
 [  7.40000000e+02   2.35551619e-03]
 [  7.50000000e+02   1.82922091e-03]
 [  7.60000000e+02   2.97129876e-03]
 [  7.70000000e+02   2.08460935e-03]
 [  7.80000000e+02   2.44995812e-03]
 [  7.90000000e+02   2.45113228e-03]
 [  8.00000000e+02   2.22971430e-03]
 [  8.10000000e+02   2.35671434e-03]
 [  8.20000000e+02   2.92396406e-03]
 [  8.30000000e+02   2.27630418e-03]
 [  8.40000000e+02   1.89890829e-03]
 [  8.50000000e+02   2.35408917e-03]
 [  8.60000000e+02   2.32181675e-03]
 [  8.70000000e+02   2.49334355e-03]
 [  8.80000000e+02   2.55632424e-03]
 [  8.90000000e+02   2.47281836e-03]
 [  9.00000000e+02   2.32568197e-03]
 [  9.10000000e+02   2.11712252e-03]
 [  9.20000000e+02   1.62985909e-03]
 [  9.30000000e+02   2.08126637e-03]
 [  9.40000000e+02   2.26301420e-03]
 [  9.50000000e+02   1.85970764e-03]
 [  9.60000000e+02   2.32217391e-03]
 [  9.70000000e+02   1.64019060e-03]
 [  9.80000000e+02   2.02045729e-03]
 [  9.90000000e+02   2.10024999e-03]
 [  1.00000000e+03   1.65149057e-03]
 [  1.01000000e+03   1.91835407e-03]
 [  1.02000000e+03   1.62408198e-03]
 [  1.03000000e+03   1.51674706e-03]
 [  1.04000000e+03   1.79473171e-03]
 [  1.05000000e+03   1.49998779e-03]
 [  1.06000000e+03   1.65460748e-03]
 [  1.07000000e+03   2.03388790e-03]
 [  1.08000000e+03   1.93317619e-03]
 [  1.09000000e+03   1.90710102e-03]
 [  1.10000000e+03   2.19604676e-03]
 [  1.11000000e+03   1.50470575e-03]
 [  1.12000000e+03   1.95620069e-03]
 [  1.13000000e+03   1.86586753e-03]
 [  1.14000000e+03   1.35002483e-03]
 [  1.15000000e+03   2.37491005e-03]
 [  1.16000000e+03   1.65425008e-03]
 [  1.17000000e+03   1.85328012e-03]
 [  1.18000000e+03   1.94051175e-03]
 [  1.19000000e+03   1.38949859e-03]
 [  1.20000000e+03   1.41539495e-03]
 [  1.21000000e+03   1.45017623e-03]
 [  1.22000000e+03   1.49397587e-03]
 [  1.23000000e+03   1.94837095e-03]
 [  1.24000000e+03   1.45024946e-03]
 [  1.25000000e+03   1.77673053e-03]
 [  1.26000000e+03   1.58400787e-03]
 [  1.27000000e+03   1.47867377e-03]
 [  1.28000000e+03   1.80332037e-03]
 [  1.29000000e+03   1.38736365e-03]
 [  1.30000000e+03   1.22042489e-03]
 [  1.31000000e+03   1.28827943e-03]
 [  1.32000000e+03   1.31311174e-03]
 [  1.33000000e+03   1.26408192e-03]
 [  1.34000000e+03   1.21319108e-03]
 [  1.35000000e+03   1.52061880e-03]
 [  1.36000000e+03   1.18865701e-03]
 [  1.37000000e+03   1.38971244e-03]
 [  1.38000000e+03   1.33357779e-03]
 [  1.39000000e+03   1.29566342e-03]
 [  1.40000000e+03   1.62974955e-03]
 [  1.41000000e+03   1.22151722e-03]
 [  1.42000000e+03   1.22268195e-03]
 [  1.43000000e+03   1.17790559e-03]
 [  1.44000000e+03   1.24149595e-03]
 [  1.45000000e+03   1.44640659e-03]
 [  1.46000000e+03   1.02989399e-03]
 [  1.47000000e+03   1.11122790e-03]
 [  1.48000000e+03   1.23555411e-03]
 [  1.49000000e+03   1.15039677e-03]
 [  1.50000000e+03   1.23518868e-03]
 [  1.51000000e+03   1.23302802e-03]
 [  1.52000000e+03   1.18085451e-03]
 [  1.53000000e+03   1.47570984e-03]
 [  1.54000000e+03   1.17384631e-03]
 [  1.55000000e+03   9.56637203e-04]
 [  1.56000000e+03   1.18612649e-03]
 [  1.57000000e+03   1.24448701e-03]
 [  1.58000000e+03   1.10593776e-03]
 [  1.59000000e+03   9.95729235e-04]
 [  1.60000000e+03   9.26991925e-04]
 [  1.61000000e+03   1.38370320e-03]
 [  1.62000000e+03   1.25816732e-03]
 [  1.63000000e+03   1.02042872e-03]
 [  1.64000000e+03   1.48986350e-03]
 [  1.65000000e+03   8.83683388e-04]
 [  1.66000000e+03   1.18476793e-03]
 [  1.67000000e+03   9.61380953e-04]
 [  1.68000000e+03   8.78513965e-04]
 [  1.69000000e+03   9.38194746e-04]
 [  1.70000000e+03   1.34150521e-03]
 [  1.71000000e+03   8.10550759e-04]
 [  1.72000000e+03   8.92015931e-04]
 [  1.73000000e+03   1.16464566e-03]
 [  1.74000000e+03   1.04963453e-03]
 [  1.75000000e+03   1.08016958e-03]
 [  1.76000000e+03   1.04651263e-03]
 [  1.77000000e+03   1.02604879e-03]
 [  1.78000000e+03   1.29214209e-03]
 [  1.79000000e+03   8.28185061e-04]
 [  1.80000000e+03   1.04267569e-03]
 [  1.81000000e+03   6.78852724e-04]
 [  1.82000000e+03   9.04220506e-04]
 [  1.83000000e+03   9.64932318e-04]
 [  1.84000000e+03   1.08271965e-03]
 [  1.85000000e+03   9.79476026e-04]
 [  1.86000000e+03   1.05643214e-03]
 [  1.87000000e+03   1.19211408e-03]
 [  1.88000000e+03   1.05968537e-03]
 [  1.89000000e+03   1.04642462e-03]
 [  1.90000000e+03   1.12252939e-03]
 [  1.91000000e+03   8.39525252e-04]
 [  1.92000000e+03   6.82258105e-04]
 [  1.93000000e+03   1.07447652e-03]
 [  1.94000000e+03   8.96543497e-04]
 [  1.95000000e+03   9.66358872e-04]
 [  1.96000000e+03   7.78164132e-04]
 [  1.97000000e+03   9.23915417e-04]
 [  1.98000000e+03   1.03913981e-03]
 [  1.99000000e+03   6.95308961e-04]
 [  2.00000000e+03   8.06271215e-04]]
initial: [  0.00000000e+00   1.25333234e-01   2.48689887e-01   3.68124553e-01
   4.81753674e-01   5.87785252e-01   6.84547106e-01   7.70513243e-01
   8.44327926e-01   9.04827052e-01   9.51056516e-01   9.82287251e-01
   9.98026728e-01   9.98026728e-01   9.82287251e-01   9.51056516e-01
   9.04827052e-01   8.44327926e-01   7.70513243e-01   6.84547106e-01
   5.87785252e-01   4.81753674e-01   3.68124553e-01   2.48689887e-01
   1.25333234e-01  -3.21624530e-16  -1.25333234e-01  -2.48689887e-01
  -3.68124553e-01  -4.81753674e-01  -5.87785252e-01  -6.84547106e-01
  -7.70513243e-01  -8.44327926e-01  -9.04827052e-01  -9.51056516e-01
  -9.82287251e-01  -9.98026728e-01  -9.98026728e-01  -9.82287251e-01
  -9.51056516e-01  -9.04827052e-01  -8.44327926e-01  -7.70513243e-01
  -6.84547106e-01  -5.87785252e-01  -4.81753674e-01  -3.68124553e-01
  -2.48689887e-01  -1.25333234e-01]
prediction#1, output: 0.006050
prediction#2, output: 0.143671
prediction#3, output: 0.281191
prediction#4, output: 0.412959
prediction#5, output: 0.534886
prediction#6, output: 0.644377
prediction#7, output: 0.739448
prediction#8, output: 0.818086
prediction#9, output: 0.878557
prediction#10, output: 0.920342
prediction#11, output: 0.944622
prediction#12, output: 0.953781
prediction#13, output: 0.950478
prediction#14, output: 0.936998
prediction#15, output: 0.915027
prediction#16, output: 0.885666
prediction#17, output: 0.849515
prediction#18, output: 0.806749
prediction#19, output: 0.757173
prediction#20, output: 0.700255
prediction#21, output: 0.635134
prediction#22, output: 0.560640
prediction#23, output: 0.475312
prediction#24, output: 0.377492
prediction#25, output: 0.265536
prediction#26, output: 0.138315
prediction#27, output: -0.003813
prediction#28, output: -0.157398
prediction#29, output: -0.314185
prediction#30, output: -0.461207
prediction#31, output: -0.584697
prediction#32, output: -0.676040
prediction#33, output: -0.734458
prediction#34, output: -0.764685
prediction#35, output: -0.773028
prediction#36, output: -0.764874
prediction#37, output: -0.743930
prediction#38, output: -0.712347
prediction#39, output: -0.671081
prediction#40, output: -0.620208
prediction#41, output: -0.559166
prediction#42, output: -0.486972
prediction#43, output: -0.402467
prediction#44, output: -0.304701
prediction#45, output: -0.193493
prediction#46, output: -0.070113
prediction#47, output: 0.062193
prediction#48, output: 0.198351
prediction#49, output: 0.332540
prediction#50, output: 0.459676
prediction#51, output: 0.576216
prediction#52, output: 0.679800
prediction#53, output: 0.768385
prediction#54, output: 0.839947
prediction#55, output: 0.893105
prediction#56, output: 0.927968
prediction#57, output: 0.946232
prediction#58, output: 0.950441
prediction#59, output: 0.943126
prediction#60, output: 0.926331
prediction#61, output: 0.901501
prediction#62, output: 0.869528
prediction#63, output: 0.830839
prediction#64, output: 0.785460
prediction#65, output: 0.733061
prediction#66, output: 0.672980
prediction#67, output: 0.604234
prediction#68, output: 0.525534
prediction#69, output: 0.435330
prediction#70, output: 0.331944
prediction#71, output: 0.213878
prediction#72, output: 0.080484
prediction#73, output: -0.066797
prediction#74, output: -0.222670
prediction#75, output: -0.376730
prediction#76, output: -0.515031
prediction#77, output: -0.625361
prediction#78, output: -0.702394
prediction#79, output: -0.748107
prediction#80, output: -0.768285
prediction#81, output: -0.769007
prediction#82, output: -0.754956
prediction#83, output: -0.729133
prediction#84, output: -0.693135
prediction#85, output: -0.647501
prediction#86, output: -0.592005
prediction#87, output: -0.525873
prediction#88, output: -0.448013
prediction#89, output: -0.357318
prediction#90, output: -0.253129
prediction#91, output: -0.135874
prediction#92, output: -0.007733
prediction#93, output: 0.127100
prediction#94, output: 0.263010
prediction#95, output: 0.394360
prediction#96, output: 0.516775
prediction#97, output: 0.627379
prediction#98, output: 0.724055
prediction#99, output: 0.804746
prediction#100, output: 0.867616
outputs: [ 0.00605029  0.1436711   0.28119054  0.41295895  0.53488636  0.64437664
  0.73944843  0.81808639  0.87855673  0.92034185  0.94462228  0.95378113
  0.95047808  0.93699789  0.9150269   0.88566625  0.84951532  0.80674911
  0.75717348  0.70025468  0.63513434  0.56063968  0.47531199  0.3774915
  0.26553637  0.13831532 -0.00381312 -0.1573976  -0.31418517 -0.46120694
 -0.58469665 -0.67603981 -0.73445821 -0.76468468 -0.77302766 -0.7648741
 -0.74392951 -0.71234703 -0.6710813  -0.62020802 -0.55916643 -0.48697165
 -0.40246674 -0.30470136 -0.19349307 -0.0701133   0.06219277  0.19835132
  0.33254027  0.45967573  0.57621598  0.67979968  0.76838458  0.8399471
  0.89310539  0.92796803  0.94623172  0.95044065  0.94312584  0.92633128
  0.90150094  0.86952829  0.83083916  0.78546     0.73306108  0.67298043
  0.60423446  0.52553409  0.43533027  0.33194381  0.21387765  0.08048415
 -0.06679717 -0.22267023 -0.37672976 -0.51503086 -0.62536073 -0.70239377
 -0.74810684 -0.76828504 -0.76900721 -0.75495589 -0.72913325 -0.69313455
 -0.64750099 -0.59200454 -0.52587283 -0.44801334 -0.35731843 -0.25312886
 -0.13587379 -0.0077326   0.12709972  0.2630097   0.39435956  0.51677513
  0.62737918  0.72405541  0.80474603  0.86761582]

real	4m25.032s
user	2m37.054s
sys	3m51.429s
