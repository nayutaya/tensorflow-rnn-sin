param_path: param.yaml
length_of_sequences: 40
forget_bias: 1.0
train_data_path: ../train_data/normal.npy
optimizer: GradientDescentOptimizer
num_of_input_nodes: 1
num_of_training_epochs: 2000
num_of_output_nodes: 1
num_of_hidden_nodes: 2
seed: 0
learning_rate: 0.1
size_of_mini_batch: 100
num_of_prediction_epochs: 100
train_data: [[  0.00000000e+00   1.25333234e-01]
 [  1.25333234e-01   2.48689887e-01]
 [  2.48689887e-01   3.68124553e-01]
 ..., 
 [ -3.68124553e-01  -2.48689887e-01]
 [ -2.48689887e-01  -1.25333234e-01]
 [ -1.25333234e-01   3.92877345e-15]]
train#10, train loss: 4.735264e-01
train#20, train loss: 5.082577e-01
train#30, train loss: 4.547597e-01
train#40, train loss: 2.802282e-01
train#50, train loss: 1.651382e-01
train#60, train loss: 1.094630e-01
train#70, train loss: 7.653806e-02
train#80, train loss: 6.265702e-02
train#90, train loss: 4.479406e-02
train#100, train loss: 4.651859e-02
train#110, train loss: 3.543216e-02
train#120, train loss: 2.851492e-02
train#130, train loss: 2.118327e-02
train#140, train loss: 1.797774e-02
train#150, train loss: 1.717402e-02
train#160, train loss: 1.817293e-02
train#170, train loss: 1.429804e-02
train#180, train loss: 1.551911e-02
train#190, train loss: 1.297506e-02
train#200, train loss: 1.272271e-02
train#210, train loss: 1.072498e-02
train#220, train loss: 1.077718e-02
train#230, train loss: 9.323703e-03
train#240, train loss: 9.344756e-03
train#250, train loss: 7.007584e-03
train#260, train loss: 6.878057e-03
train#270, train loss: 5.276042e-03
train#280, train loss: 5.855813e-03
train#290, train loss: 4.367042e-03
train#300, train loss: 3.691022e-03
train#310, train loss: 5.061357e-03
train#320, train loss: 4.992575e-03
train#330, train loss: 3.328565e-03
train#340, train loss: 2.868395e-02
train#350, train loss: 3.137943e-03
train#360, train loss: 2.576839e-03
train#370, train loss: 2.841935e-03
train#380, train loss: 2.577695e-03
train#390, train loss: 2.436733e-03
train#400, train loss: 1.957358e-03
train#410, train loss: 2.089212e-03
train#420, train loss: 3.673739e-03
train#430, train loss: 9.196630e-03
train#440, train loss: 2.201431e-03
train#450, train loss: 1.797577e-03
train#460, train loss: 1.606594e-03
train#470, train loss: 1.766849e-03
train#480, train loss: 1.264948e-03
train#490, train loss: 1.303184e-03
train#500, train loss: 1.336467e-03
train#510, train loss: 1.317522e-03
train#520, train loss: 1.077820e-03
train#530, train loss: 4.758998e-03
train#540, train loss: 1.346025e-03
train#550, train loss: 1.278448e-03
train#560, train loss: 1.418433e-03
train#570, train loss: 9.648314e-04
train#580, train loss: 1.161785e-03
train#590, train loss: 1.220236e-03
train#600, train loss: 1.062130e-03
train#610, train loss: 1.071744e-03
train#620, train loss: 9.282061e-04
train#630, train loss: 8.455055e-04
train#640, train loss: 9.456254e-04
train#650, train loss: 1.347265e-03
train#660, train loss: 8.428451e-04
train#670, train loss: 7.233990e-04
train#680, train loss: 7.925424e-04
train#690, train loss: 7.576705e-04
train#700, train loss: 7.357112e-04
train#710, train loss: 7.275942e-04
train#720, train loss: 9.242809e-04
train#730, train loss: 1.183038e-03
train#740, train loss: 7.047874e-04
train#750, train loss: 9.247506e-04
train#760, train loss: 6.575444e-04
train#770, train loss: 9.279807e-04
train#780, train loss: 6.521568e-04
train#790, train loss: 6.195965e-04
train#800, train loss: 6.300845e-04
train#810, train loss: 5.634839e-04
train#820, train loss: 6.066619e-04
train#830, train loss: 6.272195e-04
train#840, train loss: 5.342822e-04
train#850, train loss: 5.033875e-04
train#860, train loss: 5.506700e-04
train#870, train loss: 5.092305e-04
train#880, train loss: 5.224224e-04
train#890, train loss: 4.843702e-04
train#900, train loss: 4.985570e-04
train#910, train loss: 5.482931e-04
train#920, train loss: 5.099959e-04
train#930, train loss: 4.254500e-04
train#940, train loss: 4.507361e-04
train#950, train loss: 4.755626e-04
train#960, train loss: 3.986785e-04
train#970, train loss: 6.716106e-04
train#980, train loss: 4.379083e-04
train#990, train loss: 4.955547e-04
train#1000, train loss: 3.852239e-04
train#1010, train loss: 5.433064e-04
train#1020, train loss: 3.482185e-04
train#1030, train loss: 5.091032e-04
train#1040, train loss: 3.790282e-04
train#1050, train loss: 3.904797e-04
train#1060, train loss: 4.933603e-04
train#1070, train loss: 4.403687e-04
train#1080, train loss: 3.448279e-04
train#1090, train loss: 3.678529e-04
train#1100, train loss: 3.398524e-04
train#1110, train loss: 3.686915e-04
train#1120, train loss: 3.663940e-04
train#1130, train loss: 3.532956e-04
train#1140, train loss: 3.258273e-04
train#1150, train loss: 3.813918e-04
train#1160, train loss: 4.335187e-04
train#1170, train loss: 4.654352e-04
train#1180, train loss: 2.792543e-04
train#1190, train loss: 3.148760e-04
train#1200, train loss: 3.626975e-04
train#1210, train loss: 3.589724e-04
train#1220, train loss: 3.810300e-04
train#1230, train loss: 3.614395e-04
train#1240, train loss: 2.950017e-04
train#1250, train loss: 2.546182e-04
train#1260, train loss: 3.918707e-04
train#1270, train loss: 3.268530e-04
train#1280, train loss: 3.104886e-04
train#1290, train loss: 2.823499e-04
train#1300, train loss: 2.802288e-04
train#1310, train loss: 3.098094e-04
train#1320, train loss: 3.499956e-04
train#1330, train loss: 2.665101e-04
train#1340, train loss: 2.518061e-04
train#1350, train loss: 2.606084e-04
train#1360, train loss: 2.887576e-04
train#1370, train loss: 3.116350e-04
train#1380, train loss: 2.892366e-04
train#1390, train loss: 2.722737e-04
train#1400, train loss: 3.287271e-04
train#1410, train loss: 2.598896e-04
train#1420, train loss: 2.865956e-04
train#1430, train loss: 2.515650e-04
train#1440, train loss: 2.392282e-04
train#1450, train loss: 2.555206e-04
train#1460, train loss: 2.506655e-04
train#1470, train loss: 3.059006e-04
train#1480, train loss: 2.272652e-04
train#1490, train loss: 2.418198e-04
train#1500, train loss: 2.430785e-04
train#1510, train loss: 2.931116e-04
train#1520, train loss: 2.634347e-04
train#1530, train loss: 2.562929e-04
train#1540, train loss: 2.577233e-04
train#1550, train loss: 2.131201e-04
train#1560, train loss: 2.299575e-04
train#1570, train loss: 2.943784e-04
train#1580, train loss: 2.378855e-04
train#1590, train loss: 2.301675e-04
train#1600, train loss: 2.643787e-04
train#1610, train loss: 2.523517e-04
train#1620, train loss: 2.249118e-04
train#1630, train loss: 2.577362e-04
train#1640, train loss: 2.244633e-04
train#1650, train loss: 2.657134e-04
train#1660, train loss: 2.421245e-04
train#1670, train loss: 2.508038e-04
train#1680, train loss: 2.173294e-04
train#1690, train loss: 1.911699e-04
train#1700, train loss: 2.392491e-04
train#1710, train loss: 2.128517e-04
train#1720, train loss: 2.250700e-04
train#1730, train loss: 2.182292e-04
train#1740, train loss: 2.291775e-04
train#1750, train loss: 2.715433e-04
train#1760, train loss: 2.190510e-04
train#1770, train loss: 2.275837e-04
train#1780, train loss: 2.368224e-04
train#1790, train loss: 2.281067e-04
train#1800, train loss: 2.270669e-04
train#1810, train loss: 2.142277e-04
train#1820, train loss: 2.160909e-04
train#1830, train loss: 2.090242e-04
train#1840, train loss: 2.216439e-04
train#1850, train loss: 1.967085e-04
train#1860, train loss: 1.787332e-04
train#1870, train loss: 1.874708e-04
train#1880, train loss: 1.906607e-04
train#1890, train loss: 1.955010e-04
train#1900, train loss: 2.323132e-04
train#1910, train loss: 1.799749e-04
train#1920, train loss: 2.055003e-04
train#1930, train loss: 1.858616e-04
train#1940, train loss: 2.309468e-04
train#1950, train loss: 1.695690e-04
train#1960, train loss: 1.685434e-04
train#1970, train loss: 2.208250e-04
train#1980, train loss: 2.188020e-04
train#1990, train loss: 2.109837e-04
train#2000, train loss: 1.910299e-04
losses: [[  1.00000000e+01   4.73526448e-01]
 [  2.00000000e+01   5.08257747e-01]
 [  3.00000000e+01   4.54759717e-01]
 [  4.00000000e+01   2.80228168e-01]
 [  5.00000000e+01   1.65138155e-01]
 [  6.00000000e+01   1.09463006e-01]
 [  7.00000000e+01   7.65380561e-02]
 [  8.00000000e+01   6.26570210e-02]
 [  9.00000000e+01   4.47940640e-02]
 [  1.00000000e+02   4.65185940e-02]
 [  1.10000000e+02   3.54321599e-02]
 [  1.20000000e+02   2.85149198e-02]
 [  1.30000000e+02   2.11832710e-02]
 [  1.40000000e+02   1.79777406e-02]
 [  1.50000000e+02   1.71740204e-02]
 [  1.60000000e+02   1.81729309e-02]
 [  1.70000000e+02   1.42980386e-02]
 [  1.80000000e+02   1.55191096e-02]
 [  1.90000000e+02   1.29750585e-02]
 [  2.00000000e+02   1.27227064e-02]
 [  2.10000000e+02   1.07249832e-02]
 [  2.20000000e+02   1.07771801e-02]
 [  2.30000000e+02   9.32370313e-03]
 [  2.40000000e+02   9.34475567e-03]
 [  2.50000000e+02   7.00758444e-03]
 [  2.60000000e+02   6.87805656e-03]
 [  2.70000000e+02   5.27604204e-03]
 [  2.80000000e+02   5.85581316e-03]
 [  2.90000000e+02   4.36704187e-03]
 [  3.00000000e+02   3.69102159e-03]
 [  3.10000000e+02   5.06135682e-03]
 [  3.20000000e+02   4.99257492e-03]
 [  3.30000000e+02   3.32856528e-03]
 [  3.40000000e+02   2.86839548e-02]
 [  3.50000000e+02   3.13794264e-03]
 [  3.60000000e+02   2.57683941e-03]
 [  3.70000000e+02   2.84193526e-03]
 [  3.80000000e+02   2.57769506e-03]
 [  3.90000000e+02   2.43673287e-03]
 [  4.00000000e+02   1.95735763e-03]
 [  4.10000000e+02   2.08921172e-03]
 [  4.20000000e+02   3.67373903e-03]
 [  4.30000000e+02   9.19662975e-03]
 [  4.40000000e+02   2.20143120e-03]
 [  4.50000000e+02   1.79757690e-03]
 [  4.60000000e+02   1.60659384e-03]
 [  4.70000000e+02   1.76684896e-03]
 [  4.80000000e+02   1.26494805e-03]
 [  4.90000000e+02   1.30318431e-03]
 [  5.00000000e+02   1.33646699e-03]
 [  5.10000000e+02   1.31752167e-03]
 [  5.20000000e+02   1.07782008e-03]
 [  5.30000000e+02   4.75899782e-03]
 [  5.40000000e+02   1.34602515e-03]
 [  5.50000000e+02   1.27844769e-03]
 [  5.60000000e+02   1.41843292e-03]
 [  5.70000000e+02   9.64831386e-04]
 [  5.80000000e+02   1.16178463e-03]
 [  5.90000000e+02   1.22023595e-03]
 [  6.00000000e+02   1.06213009e-03]
 [  6.10000000e+02   1.07174413e-03]
 [  6.20000000e+02   9.28206078e-04]
 [  6.30000000e+02   8.45505507e-04]
 [  6.40000000e+02   9.45625361e-04]
 [  6.50000000e+02   1.34726497e-03]
 [  6.60000000e+02   8.42845067e-04]
 [  6.70000000e+02   7.23399047e-04]
 [  6.80000000e+02   7.92542414e-04]
 [  6.90000000e+02   7.57670496e-04]
 [  7.00000000e+02   7.35711248e-04]
 [  7.10000000e+02   7.27594190e-04]
 [  7.20000000e+02   9.24280903e-04]
 [  7.30000000e+02   1.18303846e-03]
 [  7.40000000e+02   7.04787439e-04]
 [  7.50000000e+02   9.24750580e-04]
 [  7.60000000e+02   6.57544355e-04]
 [  7.70000000e+02   9.27980698e-04]
 [  7.80000000e+02   6.52156770e-04]
 [  7.90000000e+02   6.19596511e-04]
 [  8.00000000e+02   6.30084483e-04]
 [  8.10000000e+02   5.63483918e-04]
 [  8.20000000e+02   6.06661895e-04]
 [  8.30000000e+02   6.27219502e-04]
 [  8.40000000e+02   5.34282241e-04]
 [  8.50000000e+02   5.03387477e-04]
 [  8.60000000e+02   5.50669967e-04]
 [  8.70000000e+02   5.09230536e-04]
 [  8.80000000e+02   5.22422371e-04]
 [  8.90000000e+02   4.84370219e-04]
 [  9.00000000e+02   4.98556998e-04]
 [  9.10000000e+02   5.48293116e-04]
 [  9.20000000e+02   5.09995909e-04]
 [  9.30000000e+02   4.25449980e-04]
 [  9.40000000e+02   4.50736057e-04]
 [  9.50000000e+02   4.75562614e-04]
 [  9.60000000e+02   3.98678472e-04]
 [  9.70000000e+02   6.71610585e-04]
 [  9.80000000e+02   4.37908253e-04]
 [  9.90000000e+02   4.95554705e-04]
 [  1.00000000e+03   3.85223917e-04]
 [  1.01000000e+03   5.43306407e-04]
 [  1.02000000e+03   3.48218542e-04]
 [  1.03000000e+03   5.09103178e-04]
 [  1.04000000e+03   3.79028235e-04]
 [  1.05000000e+03   3.90479719e-04]
 [  1.06000000e+03   4.93360334e-04]
 [  1.07000000e+03   4.40368720e-04]
 [  1.08000000e+03   3.44827888e-04]
 [  1.09000000e+03   3.67852888e-04]
 [  1.10000000e+03   3.39852355e-04]
 [  1.11000000e+03   3.68691457e-04]
 [  1.12000000e+03   3.66393971e-04]
 [  1.13000000e+03   3.53295647e-04]
 [  1.14000000e+03   3.25827277e-04]
 [  1.15000000e+03   3.81391845e-04]
 [  1.16000000e+03   4.33518668e-04]
 [  1.17000000e+03   4.65435238e-04]
 [  1.18000000e+03   2.79254280e-04]
 [  1.19000000e+03   3.14876001e-04]
 [  1.20000000e+03   3.62697465e-04]
 [  1.21000000e+03   3.58972407e-04]
 [  1.22000000e+03   3.81029997e-04]
 [  1.23000000e+03   3.61439510e-04]
 [  1.24000000e+03   2.95001693e-04]
 [  1.25000000e+03   2.54618208e-04]
 [  1.26000000e+03   3.91870708e-04]
 [  1.27000000e+03   3.26853013e-04]
 [  1.28000000e+03   3.10488598e-04]
 [  1.29000000e+03   2.82349909e-04]
 [  1.30000000e+03   2.80228793e-04]
 [  1.31000000e+03   3.09809431e-04]
 [  1.32000000e+03   3.49995593e-04]
 [  1.33000000e+03   2.66510091e-04]
 [  1.34000000e+03   2.51806137e-04]
 [  1.35000000e+03   2.60608416e-04]
 [  1.36000000e+03   2.88757612e-04]
 [  1.37000000e+03   3.11634998e-04]
 [  1.38000000e+03   2.89236574e-04]
 [  1.39000000e+03   2.72273726e-04]
 [  1.40000000e+03   3.28727096e-04]
 [  1.41000000e+03   2.59889581e-04]
 [  1.42000000e+03   2.86595605e-04]
 [  1.43000000e+03   2.51565041e-04]
 [  1.44000000e+03   2.39228160e-04]
 [  1.45000000e+03   2.55520601e-04]
 [  1.46000000e+03   2.50665529e-04]
 [  1.47000000e+03   3.05900641e-04]
 [  1.48000000e+03   2.27265205e-04]
 [  1.49000000e+03   2.41819784e-04]
 [  1.50000000e+03   2.43078539e-04]
 [  1.51000000e+03   2.93111574e-04]
 [  1.52000000e+03   2.63434660e-04]
 [  1.53000000e+03   2.56292871e-04]
 [  1.54000000e+03   2.57723266e-04]
 [  1.55000000e+03   2.13120089e-04]
 [  1.56000000e+03   2.29957514e-04]
 [  1.57000000e+03   2.94378435e-04]
 [  1.58000000e+03   2.37885528e-04]
 [  1.59000000e+03   2.30167512e-04]
 [  1.60000000e+03   2.64378701e-04]
 [  1.61000000e+03   2.52351747e-04]
 [  1.62000000e+03   2.24911753e-04]
 [  1.63000000e+03   2.57736188e-04]
 [  1.64000000e+03   2.24463292e-04]
 [  1.65000000e+03   2.65713403e-04]
 [  1.66000000e+03   2.42124515e-04]
 [  1.67000000e+03   2.50803801e-04]
 [  1.68000000e+03   2.17329391e-04]
 [  1.69000000e+03   1.91169907e-04]
 [  1.70000000e+03   2.39249115e-04]
 [  1.71000000e+03   2.12851694e-04]
 [  1.72000000e+03   2.25069976e-04]
 [  1.73000000e+03   2.18229165e-04]
 [  1.74000000e+03   2.29177545e-04]
 [  1.75000000e+03   2.71543307e-04]
 [  1.76000000e+03   2.19050969e-04]
 [  1.77000000e+03   2.27583718e-04]
 [  1.78000000e+03   2.36822438e-04]
 [  1.79000000e+03   2.28106714e-04]
 [  1.80000000e+03   2.27066877e-04]
 [  1.81000000e+03   2.14227737e-04]
 [  1.82000000e+03   2.16090906e-04]
 [  1.83000000e+03   2.09024176e-04]
 [  1.84000000e+03   2.21643873e-04]
 [  1.85000000e+03   1.96708497e-04]
 [  1.86000000e+03   1.78733200e-04]
 [  1.87000000e+03   1.87470840e-04]
 [  1.88000000e+03   1.90660707e-04]
 [  1.89000000e+03   1.95501038e-04]
 [  1.90000000e+03   2.32313207e-04]
 [  1.91000000e+03   1.79974886e-04]
 [  1.92000000e+03   2.05500313e-04]
 [  1.93000000e+03   1.85861587e-04]
 [  1.94000000e+03   2.30946796e-04]
 [  1.95000000e+03   1.69568957e-04]
 [  1.96000000e+03   1.68543440e-04]
 [  1.97000000e+03   2.20824964e-04]
 [  1.98000000e+03   2.18802015e-04]
 [  1.99000000e+03   2.10983664e-04]
 [  2.00000000e+03   1.91029903e-04]]
initial: [  0.00000000e+00   1.25333234e-01   2.48689887e-01   3.68124553e-01
   4.81753674e-01   5.87785252e-01   6.84547106e-01   7.70513243e-01
   8.44327926e-01   9.04827052e-01   9.51056516e-01   9.82287251e-01
   9.98026728e-01   9.98026728e-01   9.82287251e-01   9.51056516e-01
   9.04827052e-01   8.44327926e-01   7.70513243e-01   6.84547106e-01
   5.87785252e-01   4.81753674e-01   3.68124553e-01   2.48689887e-01
   1.25333234e-01  -3.21624530e-16  -1.25333234e-01  -2.48689887e-01
  -3.68124553e-01  -4.81753674e-01  -5.87785252e-01  -6.84547106e-01
  -7.70513243e-01  -8.44327926e-01  -9.04827052e-01  -9.51056516e-01
  -9.82287251e-01  -9.98026728e-01  -9.98026728e-01  -9.82287251e-01]
prediction#1, output: -0.953325
prediction#2, output: -0.920118
prediction#3, output: -0.870679
prediction#4, output: -0.806735
prediction#5, output: -0.726650
prediction#6, output: -0.629505
prediction#7, output: -0.516084
prediction#8, output: -0.390278
prediction#9, output: -0.259414
prediction#10, output: -0.131798
prediction#11, output: -0.012290
prediction#12, output: 0.099840
prediction#13, output: 0.208764
prediction#14, output: 0.318567
prediction#15, output: 0.431006
prediction#16, output: 0.544692
prediction#17, output: 0.655307
prediction#18, output: 0.756803
prediction#19, output: 0.843327
prediction#20, output: 0.910966
prediction#21, output: 0.958390
prediction#22, output: 0.986349
prediction#23, output: 0.996658
prediction#24, output: 0.991321
prediction#25, output: 0.972020
prediction#26, output: 0.939928
prediction#27, output: 0.895724
prediction#28, output: 0.839719
prediction#29, output: 0.772047
prediction#30, output: 0.692876
prediction#31, output: 0.602600
prediction#32, output: 0.501956
prediction#33, output: 0.392056
prediction#34, output: 0.274360
prediction#35, output: 0.150642
prediction#36, output: 0.022984
prediction#37, output: -0.106231
prediction#38, output: -0.234359
prediction#39, output: -0.358585
prediction#40, output: -0.476076
prediction#41, output: -0.584187
prediction#42, output: -0.680684
prediction#43, output: -0.763930
prediction#44, output: -0.832973
prediction#45, output: -0.887521
prediction#46, output: -0.927796
prediction#47, output: -0.954336
prediction#48, output: -0.967782
prediction#49, output: -0.968679
prediction#50, output: -0.957324
prediction#51, output: -0.933650
prediction#52, output: -0.897146
prediction#53, output: -0.846851
prediction#54, output: -0.781444
prediction#55, output: -0.699571
prediction#56, output: -0.600574
prediction#57, output: -0.485711
prediction#58, output: -0.359445
prediction#59, output: -0.229360
prediction#60, output: -0.103269
prediction#61, output: 0.014982
prediction#62, output: 0.126838
prediction#63, output: 0.236456
prediction#64, output: 0.347369
prediction#65, output: 0.460559
prediction#66, output: 0.573857
prediction#67, output: 0.682403
prediction#68, output: 0.780086
prediction#69, output: 0.861506
prediction#70, output: 0.923476
prediction#71, output: 0.965325
prediction#72, output: 0.988186
prediction#73, output: 0.993984
prediction#74, output: 0.984658
prediction#75, output: 0.961752
prediction#76, output: 0.926290
prediction#77, output: 0.878831
prediction#78, output: 0.819614
prediction#79, output: 0.748764
prediction#80, output: 0.666494
prediction#81, output: 0.573291
prediction#82, output: 0.469998
prediction#83, output: 0.357833
prediction#84, output: 0.238349
prediction#85, output: 0.113408
prediction#86, output: -0.014820
prediction#87, output: -0.143874
prediction#88, output: -0.271050
prediction#89, output: -0.393513
prediction#90, output: -0.508459
prediction#91, output: -0.613340
prediction#92, output: -0.706076
prediction#93, output: -0.785223
prediction#94, output: -0.850029
prediction#95, output: -0.900366
prediction#96, output: -0.936567
prediction#97, output: -0.959221
prediction#98, output: -0.968954
prediction#99, output: -0.966253
prediction#100, output: -0.951318
outputs: [-0.95332503 -0.92011762 -0.87067926 -0.80673462 -0.72664982 -0.6295045
 -0.51608354 -0.39027759 -0.25941375 -0.13179824 -0.01229022  0.09983977
  0.2087644   0.31856743  0.43100587  0.54469228  0.65530688  0.75680256
  0.8433274   0.91096622  0.95839024  0.98634923  0.99665821  0.99132085
  0.97201979  0.939928    0.89572382  0.83971882  0.77204663  0.69287592
  0.6026001   0.50195622  0.39205647  0.27436021  0.15064213  0.02298426
 -0.10623097 -0.23435897 -0.35858494 -0.47607589 -0.58418685 -0.68068433
 -0.7639299  -0.83297348 -0.88752097 -0.92779565 -0.9543364  -0.96778202
 -0.96867907 -0.95732439 -0.93364966 -0.89714622 -0.84685129 -0.78144413
 -0.69957149 -0.60057408 -0.48571104 -0.35944456 -0.22935978 -0.10326896
  0.01498237  0.12683848  0.23645565  0.34736943  0.46055913  0.57385713
  0.68240333  0.7800858   0.86150599  0.92347634  0.96532542  0.98818612
  0.99398392  0.98465836  0.9617523   0.92629033  0.87883079  0.81961417
  0.7487635   0.66649449  0.57329082  0.46999818  0.35783315  0.23834869
  0.11340808 -0.01481996 -0.14387387 -0.27105039 -0.39351258 -0.50845891
 -0.61333954 -0.70607579 -0.78522331 -0.85002929 -0.90036577 -0.93656743
 -0.95922077 -0.96895397 -0.9662534  -0.95131803]

real	2m58.636s
user	1m45.613s
sys	1m48.508s
