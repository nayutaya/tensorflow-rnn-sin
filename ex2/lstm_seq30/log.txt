param_path: param.yaml
forget_bias: 1.0
num_of_prediction_epochs: 100
num_of_output_nodes: 1
num_of_hidden_nodes: 2
num_of_training_epochs: 2000
num_of_input_nodes: 1
learning_rate: 0.1
seed: 0
length_of_sequences: 30
optimizer: GradientDescentOptimizer
size_of_mini_batch: 100
train_data_path: ../train_data/normal.npy
train_data: [[  0.00000000e+00   1.25333234e-01]
 [  1.25333234e-01   2.48689887e-01]
 [  2.48689887e-01   3.68124553e-01]
 ..., 
 [ -3.68124553e-01  -2.48689887e-01]
 [ -2.48689887e-01  -1.25333234e-01]
 [ -1.25333234e-01   3.92877345e-15]]
train#10, train loss: 5.142992e-01
train#20, train loss: 4.060519e-01
train#30, train loss: 2.985763e-01
train#40, train loss: 2.315473e-01
train#50, train loss: 1.635803e-01
train#60, train loss: 1.206251e-01
train#70, train loss: 8.644564e-02
train#80, train loss: 6.484043e-02
train#90, train loss: 5.510279e-02
train#100, train loss: 3.805321e-02
train#110, train loss: 3.155467e-02
train#120, train loss: 2.286888e-02
train#130, train loss: 1.744514e-02
train#140, train loss: 1.581009e-02
train#150, train loss: 1.180940e-02
train#160, train loss: 8.991424e-03
train#170, train loss: 7.772081e-03
train#180, train loss: 7.641241e-03
train#190, train loss: 5.901494e-03
train#200, train loss: 5.600206e-03
train#210, train loss: 6.779294e-03
train#220, train loss: 5.054606e-03
train#230, train loss: 4.139137e-03
train#240, train loss: 4.516382e-03
train#250, train loss: 4.688066e-03
train#260, train loss: 4.166389e-03
train#270, train loss: 4.106829e-03
train#280, train loss: 3.968435e-03
train#290, train loss: 4.442433e-03
train#300, train loss: 3.751435e-03
train#310, train loss: 3.838009e-03
train#320, train loss: 4.646781e-03
train#330, train loss: 3.690355e-03
train#340, train loss: 3.198175e-03
train#350, train loss: 3.105824e-03
train#360, train loss: 3.481849e-03
train#370, train loss: 3.109672e-03
train#380, train loss: 3.229494e-03
train#390, train loss: 3.203223e-03
train#400, train loss: 2.799732e-03
train#410, train loss: 3.002642e-03
train#420, train loss: 3.119749e-03
train#430, train loss: 2.643102e-03
train#440, train loss: 2.816207e-03
train#450, train loss: 3.012434e-03
train#460, train loss: 2.872741e-03
train#470, train loss: 2.829325e-03
train#480, train loss: 2.679809e-03
train#490, train loss: 2.881249e-03
train#500, train loss: 2.071360e-03
train#510, train loss: 2.457338e-03
train#520, train loss: 2.277651e-03
train#530, train loss: 2.096422e-03
train#540, train loss: 2.433648e-03
train#550, train loss: 2.159638e-03
train#560, train loss: 1.903832e-03
train#570, train loss: 2.543977e-03
train#580, train loss: 2.445953e-03
train#590, train loss: 2.404914e-03
train#600, train loss: 2.135871e-03
train#610, train loss: 2.114493e-03
train#620, train loss: 2.494990e-03
train#630, train loss: 2.296236e-03
train#640, train loss: 2.077982e-03
train#650, train loss: 2.208035e-03
train#660, train loss: 1.963171e-03
train#670, train loss: 2.044291e-03
train#680, train loss: 2.122955e-03
train#690, train loss: 2.107593e-03
train#700, train loss: 2.224130e-03
train#710, train loss: 1.643595e-03
train#720, train loss: 1.771840e-03
train#730, train loss: 2.179232e-03
train#740, train loss: 1.667245e-03
train#750, train loss: 1.962179e-03
train#760, train loss: 2.065383e-03
train#770, train loss: 2.085648e-03
train#780, train loss: 1.854710e-03
train#790, train loss: 1.705602e-03
train#800, train loss: 1.671839e-03
train#810, train loss: 1.544153e-03
train#820, train loss: 1.743388e-03
train#830, train loss: 1.829142e-03
train#840, train loss: 1.514237e-03
train#850, train loss: 1.484997e-03
train#860, train loss: 1.668848e-03
train#870, train loss: 1.731818e-03
train#880, train loss: 1.982951e-03
train#890, train loss: 1.321012e-03
train#900, train loss: 1.913426e-03
train#910, train loss: 1.489710e-03
train#920, train loss: 1.514542e-03
train#930, train loss: 1.495955e-03
train#940, train loss: 1.536409e-03
train#950, train loss: 1.621557e-03
train#960, train loss: 1.435123e-03
train#970, train loss: 1.478416e-03
train#980, train loss: 1.727408e-03
train#990, train loss: 1.663765e-03
train#1000, train loss: 1.665148e-03
train#1010, train loss: 1.353499e-03
train#1020, train loss: 1.703789e-03
train#1030, train loss: 1.277259e-03
train#1040, train loss: 1.379808e-03
train#1050, train loss: 1.243018e-03
train#1060, train loss: 1.600622e-03
train#1070, train loss: 1.356180e-03
train#1080, train loss: 1.229348e-03
train#1090, train loss: 1.491370e-03
train#1100, train loss: 1.458514e-03
train#1110, train loss: 1.346797e-03
train#1120, train loss: 1.494706e-03
train#1130, train loss: 1.409444e-03
train#1140, train loss: 1.254736e-03
train#1150, train loss: 1.096368e-03
train#1160, train loss: 1.130089e-03
train#1170, train loss: 1.190943e-03
train#1180, train loss: 1.241830e-03
train#1190, train loss: 1.344878e-03
train#1200, train loss: 1.337251e-03
train#1210, train loss: 1.184754e-03
train#1220, train loss: 1.238084e-03
train#1230, train loss: 1.239814e-03
train#1240, train loss: 1.080279e-03
train#1250, train loss: 1.263409e-03
train#1260, train loss: 1.264212e-03
train#1270, train loss: 1.159140e-03
train#1280, train loss: 1.207967e-03
train#1290, train loss: 1.450624e-03
train#1300, train loss: 1.230715e-03
train#1310, train loss: 1.084213e-03
train#1320, train loss: 1.222340e-03
train#1330, train loss: 7.765210e-04
train#1340, train loss: 9.576021e-04
train#1350, train loss: 1.231559e-03
train#1360, train loss: 1.068123e-03
train#1370, train loss: 1.064171e-03
train#1380, train loss: 8.576822e-04
train#1390, train loss: 9.488177e-04
train#1400, train loss: 9.217933e-04
train#1410, train loss: 1.009597e-03
train#1420, train loss: 1.220301e-03
train#1430, train loss: 1.033612e-03
train#1440, train loss: 1.039320e-03
train#1450, train loss: 9.332456e-04
train#1460, train loss: 1.072387e-03
train#1470, train loss: 1.044674e-03
train#1480, train loss: 9.108486e-04
train#1490, train loss: 1.034423e-03
train#1500, train loss: 1.018220e-03
train#1510, train loss: 1.021580e-03
train#1520, train loss: 1.062710e-03
train#1530, train loss: 9.356024e-04
train#1540, train loss: 9.830892e-04
train#1550, train loss: 1.019507e-03
train#1560, train loss: 9.132461e-04
train#1570, train loss: 8.990845e-04
train#1580, train loss: 1.017205e-03
train#1590, train loss: 1.007123e-03
train#1600, train loss: 9.266615e-04
train#1610, train loss: 9.777010e-04
train#1620, train loss: 7.821991e-04
train#1630, train loss: 9.948076e-04
train#1640, train loss: 8.093513e-04
train#1650, train loss: 6.837824e-04
train#1660, train loss: 8.436014e-04
train#1670, train loss: 8.851691e-04
train#1680, train loss: 8.017017e-04
train#1690, train loss: 8.469419e-04
train#1700, train loss: 9.143563e-04
train#1710, train loss: 9.599694e-04
train#1720, train loss: 9.271044e-04
train#1730, train loss: 8.820894e-04
train#1740, train loss: 8.671939e-04
train#1750, train loss: 7.318913e-04
train#1760, train loss: 8.925378e-04
train#1770, train loss: 8.964248e-04
train#1780, train loss: 8.329714e-04
train#1790, train loss: 6.072293e-04
train#1800, train loss: 7.798078e-04
train#1810, train loss: 8.049923e-04
train#1820, train loss: 7.752788e-04
train#1830, train loss: 8.486065e-04
train#1840, train loss: 7.407591e-04
train#1850, train loss: 8.336168e-04
train#1860, train loss: 8.402672e-04
train#1870, train loss: 7.990612e-04
train#1880, train loss: 6.952343e-04
train#1890, train loss: 7.045947e-04
train#1900, train loss: 8.348800e-04
train#1910, train loss: 7.633850e-04
train#1920, train loss: 7.225536e-04
train#1930, train loss: 7.061918e-04
train#1940, train loss: 7.087013e-04
train#1950, train loss: 7.288124e-04
train#1960, train loss: 7.898437e-04
train#1970, train loss: 7.111467e-04
train#1980, train loss: 7.946139e-04
train#1990, train loss: 7.469499e-04
train#2000, train loss: 7.932550e-04
losses: [[  1.00000000e+01   5.14299214e-01]
 [  2.00000000e+01   4.06051934e-01]
 [  3.00000000e+01   2.98576325e-01]
 [  4.00000000e+01   2.31547311e-01]
 [  5.00000000e+01   1.63580269e-01]
 [  6.00000000e+01   1.20625131e-01]
 [  7.00000000e+01   8.64456370e-02]
 [  8.00000000e+01   6.48404285e-02]
 [  9.00000000e+01   5.51027879e-02]
 [  1.00000000e+02   3.80532108e-02]
 [  1.10000000e+02   3.15546654e-02]
 [  1.20000000e+02   2.28688810e-02]
 [  1.30000000e+02   1.74451396e-02]
 [  1.40000000e+02   1.58100855e-02]
 [  1.50000000e+02   1.18094012e-02]
 [  1.60000000e+02   8.99142399e-03]
 [  1.70000000e+02   7.77208107e-03]
 [  1.80000000e+02   7.64124142e-03]
 [  1.90000000e+02   5.90149360e-03]
 [  2.00000000e+02   5.60020562e-03]
 [  2.10000000e+02   6.77929400e-03]
 [  2.20000000e+02   5.05460612e-03]
 [  2.30000000e+02   4.13913652e-03]
 [  2.40000000e+02   4.51638177e-03]
 [  2.50000000e+02   4.68806643e-03]
 [  2.60000000e+02   4.16638888e-03]
 [  2.70000000e+02   4.10682894e-03]
 [  2.80000000e+02   3.96843534e-03]
 [  2.90000000e+02   4.44243336e-03]
 [  3.00000000e+02   3.75143532e-03]
 [  3.10000000e+02   3.83800874e-03]
 [  3.20000000e+02   4.64678090e-03]
 [  3.30000000e+02   3.69035546e-03]
 [  3.40000000e+02   3.19817546e-03]
 [  3.50000000e+02   3.10582435e-03]
 [  3.60000000e+02   3.48184933e-03]
 [  3.70000000e+02   3.10967211e-03]
 [  3.80000000e+02   3.22949374e-03]
 [  3.90000000e+02   3.20322299e-03]
 [  4.00000000e+02   2.79973168e-03]
 [  4.10000000e+02   3.00264242e-03]
 [  4.20000000e+02   3.11974948e-03]
 [  4.30000000e+02   2.64310185e-03]
 [  4.40000000e+02   2.81620654e-03]
 [  4.50000000e+02   3.01243388e-03]
 [  4.60000000e+02   2.87274132e-03]
 [  4.70000000e+02   2.82932469e-03]
 [  4.80000000e+02   2.67980853e-03]
 [  4.90000000e+02   2.88124895e-03]
 [  5.00000000e+02   2.07136036e-03]
 [  5.10000000e+02   2.45733769e-03]
 [  5.20000000e+02   2.27765110e-03]
 [  5.30000000e+02   2.09642225e-03]
 [  5.40000000e+02   2.43364787e-03]
 [  5.50000000e+02   2.15963810e-03]
 [  5.60000000e+02   1.90383196e-03]
 [  5.70000000e+02   2.54397746e-03]
 [  5.80000000e+02   2.44595297e-03]
 [  5.90000000e+02   2.40491400e-03]
 [  6.00000000e+02   2.13587098e-03]
 [  6.10000000e+02   2.11449293e-03]
 [  6.20000000e+02   2.49499036e-03]
 [  6.30000000e+02   2.29623588e-03]
 [  6.40000000e+02   2.07798206e-03]
 [  6.50000000e+02   2.20803521e-03]
 [  6.60000000e+02   1.96317141e-03]
 [  6.70000000e+02   2.04429054e-03]
 [  6.80000000e+02   2.12295516e-03]
 [  6.90000000e+02   2.10759323e-03]
 [  7.00000000e+02   2.22412962e-03]
 [  7.10000000e+02   1.64359505e-03]
 [  7.20000000e+02   1.77184003e-03]
 [  7.30000000e+02   2.17923173e-03]
 [  7.40000000e+02   1.66724506e-03]
 [  7.50000000e+02   1.96217862e-03]
 [  7.60000000e+02   2.06538267e-03]
 [  7.70000000e+02   2.08564755e-03]
 [  7.80000000e+02   1.85470982e-03]
 [  7.90000000e+02   1.70560169e-03]
 [  8.00000000e+02   1.67183892e-03]
 [  8.10000000e+02   1.54415308e-03]
 [  8.20000000e+02   1.74338848e-03]
 [  8.30000000e+02   1.82914222e-03]
 [  8.40000000e+02   1.51423726e-03]
 [  8.50000000e+02   1.48499699e-03]
 [  8.60000000e+02   1.66884821e-03]
 [  8.70000000e+02   1.73181831e-03]
 [  8.80000000e+02   1.98295061e-03]
 [  8.90000000e+02   1.32101239e-03]
 [  9.00000000e+02   1.91342575e-03]
 [  9.10000000e+02   1.48970960e-03]
 [  9.20000000e+02   1.51454227e-03]
 [  9.30000000e+02   1.49595540e-03]
 [  9.40000000e+02   1.53640925e-03]
 [  9.50000000e+02   1.62155693e-03]
 [  9.60000000e+02   1.43512338e-03]
 [  9.70000000e+02   1.47841638e-03]
 [  9.80000000e+02   1.72740791e-03]
 [  9.90000000e+02   1.66376506e-03]
 [  1.00000000e+03   1.66514784e-03]
 [  1.01000000e+03   1.35349890e-03]
 [  1.02000000e+03   1.70378876e-03]
 [  1.03000000e+03   1.27725897e-03]
 [  1.04000000e+03   1.37980795e-03]
 [  1.05000000e+03   1.24301843e-03]
 [  1.06000000e+03   1.60062173e-03]
 [  1.07000000e+03   1.35618006e-03]
 [  1.08000000e+03   1.22934836e-03]
 [  1.09000000e+03   1.49137015e-03]
 [  1.10000000e+03   1.45851437e-03]
 [  1.11000000e+03   1.34679675e-03]
 [  1.12000000e+03   1.49470568e-03]
 [  1.13000000e+03   1.40944438e-03]
 [  1.14000000e+03   1.25473586e-03]
 [  1.15000000e+03   1.09636795e-03]
 [  1.16000000e+03   1.13008905e-03]
 [  1.17000000e+03   1.19094318e-03]
 [  1.18000000e+03   1.24183018e-03]
 [  1.19000000e+03   1.34487834e-03]
 [  1.20000000e+03   1.33725081e-03]
 [  1.21000000e+03   1.18475361e-03]
 [  1.22000000e+03   1.23808428e-03]
 [  1.23000000e+03   1.23981433e-03]
 [  1.24000000e+03   1.08027854e-03]
 [  1.25000000e+03   1.26340892e-03]
 [  1.26000000e+03   1.26421195e-03]
 [  1.27000000e+03   1.15914049e-03]
 [  1.28000000e+03   1.20796671e-03]
 [  1.29000000e+03   1.45062362e-03]
 [  1.30000000e+03   1.23071519e-03]
 [  1.31000000e+03   1.08421291e-03]
 [  1.32000000e+03   1.22234039e-03]
 [  1.33000000e+03   7.76520988e-04]
 [  1.34000000e+03   9.57602053e-04]
 [  1.35000000e+03   1.23155920e-03]
 [  1.36000000e+03   1.06812327e-03]
 [  1.37000000e+03   1.06417050e-03]
 [  1.38000000e+03   8.57682200e-04]
 [  1.39000000e+03   9.48817702e-04]
 [  1.40000000e+03   9.21793340e-04]
 [  1.41000000e+03   1.00959733e-03]
 [  1.42000000e+03   1.22030149e-03]
 [  1.43000000e+03   1.03361229e-03]
 [  1.44000000e+03   1.03932025e-03]
 [  1.45000000e+03   9.33245581e-04]
 [  1.46000000e+03   1.07238698e-03]
 [  1.47000000e+03   1.04467419e-03]
 [  1.48000000e+03   9.10848554e-04]
 [  1.49000000e+03   1.03442254e-03]
 [  1.50000000e+03   1.01821963e-03]
 [  1.51000000e+03   1.02157961e-03]
 [  1.52000000e+03   1.06270995e-03]
 [  1.53000000e+03   9.35602351e-04]
 [  1.54000000e+03   9.83089209e-04]
 [  1.55000000e+03   1.01950683e-03]
 [  1.56000000e+03   9.13246127e-04]
 [  1.57000000e+03   8.99084494e-04]
 [  1.58000000e+03   1.01720507e-03]
 [  1.59000000e+03   1.00712280e-03]
 [  1.60000000e+03   9.26661480e-04]
 [  1.61000000e+03   9.77701042e-04]
 [  1.62000000e+03   7.82199146e-04]
 [  1.63000000e+03   9.94807575e-04]
 [  1.64000000e+03   8.09351273e-04]
 [  1.65000000e+03   6.83782389e-04]
 [  1.66000000e+03   8.43601360e-04]
 [  1.67000000e+03   8.85169080e-04]
 [  1.68000000e+03   8.01701681e-04]
 [  1.69000000e+03   8.46941897e-04]
 [  1.70000000e+03   9.14356264e-04]
 [  1.71000000e+03   9.59969359e-04]
 [  1.72000000e+03   9.27104382e-04]
 [  1.73000000e+03   8.82089371e-04]
 [  1.74000000e+03   8.67193914e-04]
 [  1.75000000e+03   7.31891312e-04]
 [  1.76000000e+03   8.92537821e-04]
 [  1.77000000e+03   8.96424812e-04]
 [  1.78000000e+03   8.32971360e-04]
 [  1.79000000e+03   6.07229304e-04]
 [  1.80000000e+03   7.79807800e-04]
 [  1.81000000e+03   8.04992334e-04]
 [  1.82000000e+03   7.75278779e-04]
 [  1.83000000e+03   8.48606520e-04]
 [  1.84000000e+03   7.40759075e-04]
 [  1.85000000e+03   8.33616825e-04]
 [  1.86000000e+03   8.40267166e-04]
 [  1.87000000e+03   7.99061207e-04]
 [  1.88000000e+03   6.95234339e-04]
 [  1.89000000e+03   7.04594713e-04]
 [  1.90000000e+03   8.34880047e-04]
 [  1.91000000e+03   7.63384975e-04]
 [  1.92000000e+03   7.22553639e-04]
 [  1.93000000e+03   7.06191815e-04]
 [  1.94000000e+03   7.08701322e-04]
 [  1.95000000e+03   7.28812360e-04]
 [  1.96000000e+03   7.89843732e-04]
 [  1.97000000e+03   7.11146742e-04]
 [  1.98000000e+03   7.94613909e-04]
 [  1.99000000e+03   7.46949925e-04]
 [  2.00000000e+03   7.93254992e-04]]
initial: [  0.00000000e+00   1.25333234e-01   2.48689887e-01   3.68124553e-01
   4.81753674e-01   5.87785252e-01   6.84547106e-01   7.70513243e-01
   8.44327926e-01   9.04827052e-01   9.51056516e-01   9.82287251e-01
   9.98026728e-01   9.98026728e-01   9.82287251e-01   9.51056516e-01
   9.04827052e-01   8.44327926e-01   7.70513243e-01   6.84547106e-01
   5.87785252e-01   4.81753674e-01   3.68124553e-01   2.48689887e-01
   1.25333234e-01  -3.21624530e-16  -1.25333234e-01  -2.48689887e-01
  -3.68124553e-01  -4.81753674e-01]
prediction#1, output: -0.603064
prediction#2, output: -0.700777
prediction#3, output: -0.782051
prediction#4, output: -0.846205
prediction#5, output: -0.893261
prediction#6, output: -0.923826
prediction#7, output: -0.939050
prediction#8, output: -0.940453
prediction#9, output: -0.929628
prediction#10, output: -0.907954
prediction#11, output: -0.876430
prediction#12, output: -0.835603
prediction#13, output: -0.785573
prediction#14, output: -0.726022
prediction#15, output: -0.656273
prediction#16, output: -0.575379
prediction#17, output: -0.482292
prediction#18, output: -0.376165
prediction#19, output: -0.256869
prediction#20, output: -0.125737
prediction#21, output: 0.013647
prediction#22, output: 0.155130
prediction#23, output: 0.290829
prediction#24, output: 0.413118
prediction#25, output: 0.516690
prediction#26, output: 0.599404
prediction#27, output: 0.661761
prediction#28, output: 0.705765
prediction#29, output: 0.733915
prediction#30, output: 0.748581
prediction#31, output: 0.751722
prediction#32, output: 0.744805
prediction#33, output: 0.728826
prediction#34, output: 0.704350
prediction#35, output: 0.671559
prediction#36, output: 0.630300
prediction#37, output: 0.580115
prediction#38, output: 0.520292
prediction#39, output: 0.449933
prediction#40, output: 0.368090
prediction#41, output: 0.273996
prediction#42, output: 0.167428
prediction#43, output: 0.049202
prediction#44, output: -0.078328
prediction#45, output: -0.211037
prediction#46, output: -0.343406
prediction#47, output: -0.469572
prediction#48, output: -0.584570
prediction#49, output: -0.685071
prediction#50, output: -0.769286
prediction#51, output: -0.836472
prediction#52, output: -0.886545
prediction#53, output: -0.919994
prediction#54, output: -0.937870
prediction#55, output: -0.941641
prediction#56, output: -0.932904
prediction#57, output: -0.913093
prediction#58, output: -0.883280
prediction#59, output: -0.844098
prediction#60, output: -0.795724
prediction#61, output: -0.737914
prediction#62, output: -0.670048
prediction#63, output: -0.591217
prediction#64, output: -0.500370
prediction#65, output: -0.396588
prediction#66, output: -0.279557
prediction#67, output: -0.150276
prediction#68, output: -0.011873
prediction#69, output: 0.129941
prediction#70, output: 0.267461
prediction#71, output: 0.392829
prediction#72, output: 0.500196
prediction#73, output: 0.586850
prediction#74, output: 0.652891
prediction#75, output: 0.700143
prediction#76, output: 0.731079
prediction#77, output: 0.748128
prediction#78, output: 0.753339
prediction#79, output: 0.748275
prediction#80, output: 0.734018
prediction#81, output: 0.711212
prediction#82, output: 0.680111
prediction#83, output: 0.640625
prediction#84, output: 0.592354
prediction#85, output: 0.534631
prediction#86, output: 0.466588
prediction#87, output: 0.387270
prediction#88, output: 0.295839
prediction#89, output: 0.191914
prediction#90, output: 0.076037
prediction#91, output: -0.049806
prediction#92, output: -0.181870
prediction#93, output: -0.314867
prediction#94, output: -0.442894
prediction#95, output: -0.560701
prediction#96, output: -0.664582
prediction#97, output: -0.752453
prediction#98, output: -0.823386
prediction#99, output: -0.877171
prediction#100, output: -0.914172
outputs: [-0.60306442 -0.70077664 -0.78205132 -0.84620517 -0.89326143 -0.92382586
 -0.93904954 -0.94045317 -0.92962795 -0.90795422 -0.8764298  -0.835603
 -0.78557295 -0.72602242 -0.65627342 -0.57537943 -0.482292   -0.37616497
 -0.25686926 -0.12573707  0.01364657  0.15513034  0.29082888  0.41311783
  0.51668996  0.59940439  0.66176069  0.70576459  0.73391479  0.74858099
  0.75172162  0.74480528  0.72882617  0.70434958  0.67155927  0.63030022
  0.5801155   0.52029175  0.44993258  0.36808991  0.27399588  0.16742785
  0.04920165 -0.07832778 -0.21103658 -0.3434065  -0.46957177 -0.58457035
 -0.68507075 -0.76928633 -0.83647203 -0.88654512 -0.91999412 -0.93787032
 -0.94164139 -0.93290448 -0.91309279 -0.8832801  -0.84409761 -0.79572409
 -0.73791397 -0.67004812 -0.59121704 -0.50036979 -0.39658767 -0.27955657
 -0.15027599 -0.01187316  0.12994076  0.2674607   0.39282864  0.5001964
  0.58685023  0.6528911   0.70014256  0.7310788   0.74812812  0.75333935
  0.74827528  0.73401803  0.7112115   0.68011087  0.64062506  0.59235406
  0.53463143  0.46658844  0.38726956  0.29583859  0.19191359  0.07603712
 -0.04980612 -0.18186958 -0.3148669  -0.44289398 -0.56070101 -0.66458201
 -0.75245315 -0.82338601 -0.87717086 -0.91417199]

real	2m29.589s
user	1m29.130s
sys	1m43.694s
