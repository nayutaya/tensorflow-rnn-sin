param_path: param.yaml
seed: 0
num_of_hidden_nodes: 1
optimizer: GradientDescentOptimizer
num_of_output_nodes: 1
num_of_training_epochs: 2000
num_of_prediction_epochs: 100
learning_rate: 0.1
length_of_sequences: 50
train_data_path: ../train_data/normal.npy
num_of_input_nodes: 1
forget_bias: 1.0
size_of_mini_batch: 100
train_data: [[  0.00000000e+00   1.25333234e-01]
 [  1.25333234e-01   2.48689887e-01]
 [  2.48689887e-01   3.68124553e-01]
 ..., 
 [ -3.68124553e-01  -2.48689887e-01]
 [ -2.48689887e-01  -1.25333234e-01]
 [ -1.25333234e-01   3.92877345e-15]]
train#10, train loss: 5.249708e-01
train#20, train loss: 5.021659e-01
train#30, train loss: 5.219763e-01
train#40, train loss: 5.392312e-01
train#50, train loss: 4.540020e-01
train#60, train loss: 4.746065e-01
train#70, train loss: 5.148666e-01
train#80, train loss: 4.631142e-01
train#90, train loss: 4.463193e-01
train#100, train loss: 5.510441e-01
train#110, train loss: 5.198931e-01
train#120, train loss: 5.522330e-01
train#130, train loss: 5.631506e-01
train#140, train loss: 5.126529e-01
train#150, train loss: 4.438625e-01
train#160, train loss: 4.547573e-01
train#170, train loss: 4.072291e-01
train#180, train loss: 2.944379e-01
train#190, train loss: 1.744383e-01
train#200, train loss: 1.172272e-01
train#210, train loss: 9.282591e-02
train#220, train loss: 8.572290e-02
train#230, train loss: 6.666248e-02
train#240, train loss: 6.393676e-02
train#250, train loss: 5.915796e-02
train#260, train loss: 4.927785e-02
train#270, train loss: 5.051975e-02
train#280, train loss: 4.175612e-02
train#290, train loss: 3.705286e-02
train#300, train loss: 3.595623e-02
train#310, train loss: 3.154683e-02
train#320, train loss: 3.341221e-02
train#330, train loss: 2.834437e-02
train#340, train loss: 2.603605e-02
train#350, train loss: 2.849025e-02
train#360, train loss: 2.855295e-02
train#370, train loss: 2.336526e-02
train#380, train loss: 2.332125e-02
train#390, train loss: 2.072435e-02
train#400, train loss: 2.186754e-02
train#410, train loss: 2.174893e-02
train#420, train loss: 1.779575e-02
train#430, train loss: 1.967659e-02
train#440, train loss: 2.105373e-02
train#450, train loss: 2.255213e-02
train#460, train loss: 2.116235e-02
train#470, train loss: 1.881188e-02
train#480, train loss: 1.799734e-02
train#490, train loss: 1.728909e-02
train#500, train loss: 1.775400e-02
train#510, train loss: 1.641536e-02
train#520, train loss: 1.706779e-02
train#530, train loss: 1.592688e-02
train#540, train loss: 1.713086e-02
train#550, train loss: 1.469350e-02
train#560, train loss: 1.357381e-02
train#570, train loss: 1.596848e-02
train#580, train loss: 1.378116e-02
train#590, train loss: 1.392216e-02
train#600, train loss: 1.330975e-02
train#610, train loss: 1.354209e-02
train#620, train loss: 1.258785e-02
train#630, train loss: 1.441196e-02
train#640, train loss: 1.318192e-02
train#650, train loss: 1.350065e-02
train#660, train loss: 1.129699e-02
train#670, train loss: 1.331855e-02
train#680, train loss: 1.281944e-02
train#690, train loss: 1.249367e-02
train#700, train loss: 1.129302e-02
train#710, train loss: 1.078636e-02
train#720, train loss: 1.209069e-02
train#730, train loss: 1.291568e-02
train#740, train loss: 1.051403e-02
train#750, train loss: 1.139942e-02
train#760, train loss: 1.005262e-02
train#770, train loss: 1.073055e-02
train#780, train loss: 1.003587e-02
train#790, train loss: 1.205494e-02
train#800, train loss: 1.207793e-02
train#810, train loss: 1.122468e-02
train#820, train loss: 1.018099e-02
train#830, train loss: 8.498242e-03
train#840, train loss: 1.117593e-02
train#850, train loss: 9.277891e-03
train#860, train loss: 1.040881e-02
train#870, train loss: 9.838615e-03
train#880, train loss: 9.287094e-03
train#890, train loss: 1.026260e-02
train#900, train loss: 8.721404e-03
train#910, train loss: 8.947279e-03
train#920, train loss: 9.433898e-03
train#930, train loss: 1.008738e-02
train#940, train loss: 8.851910e-03
train#950, train loss: 8.383824e-03
train#960, train loss: 8.182829e-03
train#970, train loss: 9.316672e-03
train#980, train loss: 9.579182e-03
train#990, train loss: 8.182359e-03
train#1000, train loss: 9.121485e-03
train#1010, train loss: 7.683711e-03
train#1020, train loss: 7.980463e-03
train#1030, train loss: 7.177050e-03
train#1040, train loss: 9.036749e-03
train#1050, train loss: 9.425765e-03
train#1060, train loss: 7.104761e-03
train#1070, train loss: 8.268509e-03
train#1080, train loss: 8.389009e-03
train#1090, train loss: 8.070692e-03
train#1100, train loss: 7.832165e-03
train#1110, train loss: 8.825641e-03
train#1120, train loss: 6.985177e-03
train#1130, train loss: 7.928428e-03
train#1140, train loss: 7.502469e-03
train#1150, train loss: 7.100987e-03
train#1160, train loss: 6.984476e-03
train#1170, train loss: 7.867245e-03
train#1180, train loss: 7.856790e-03
train#1190, train loss: 7.570531e-03
train#1200, train loss: 7.137225e-03
train#1210, train loss: 6.649814e-03
train#1220, train loss: 6.947195e-03
train#1230, train loss: 6.894193e-03
train#1240, train loss: 6.906451e-03
train#1250, train loss: 7.211164e-03
train#1260, train loss: 6.011173e-03
train#1270, train loss: 7.266025e-03
train#1280, train loss: 5.970994e-03
train#1290, train loss: 6.897106e-03
train#1300, train loss: 6.781120e-03
train#1310, train loss: 6.999556e-03
train#1320, train loss: 6.238315e-03
train#1330, train loss: 6.140732e-03
train#1340, train loss: 7.053194e-03
train#1350, train loss: 6.668157e-03
train#1360, train loss: 5.843425e-03
train#1370, train loss: 5.817082e-03
train#1380, train loss: 6.290569e-03
train#1390, train loss: 6.516968e-03
train#1400, train loss: 6.521755e-03
train#1410, train loss: 7.337862e-03
train#1420, train loss: 6.240563e-03
train#1430, train loss: 5.863945e-03
train#1440, train loss: 6.724497e-03
train#1450, train loss: 6.590214e-03
train#1460, train loss: 6.425651e-03
train#1470, train loss: 6.112209e-03
train#1480, train loss: 5.498211e-03
train#1490, train loss: 6.158254e-03
train#1500, train loss: 5.301835e-03
train#1510, train loss: 5.447019e-03
train#1520, train loss: 5.968403e-03
train#1530, train loss: 5.841021e-03
train#1540, train loss: 6.020633e-03
train#1550, train loss: 5.793903e-03
train#1560, train loss: 5.748325e-03
train#1570, train loss: 5.623261e-03
train#1580, train loss: 5.370415e-03
train#1590, train loss: 5.271193e-03
train#1600, train loss: 6.312391e-03
train#1610, train loss: 6.538065e-03
train#1620, train loss: 5.174267e-03
train#1630, train loss: 5.303789e-03
train#1640, train loss: 5.912274e-03
train#1650, train loss: 5.021331e-03
train#1660, train loss: 4.648969e-03
train#1670, train loss: 5.386655e-03
train#1680, train loss: 5.349023e-03
train#1690, train loss: 5.290498e-03
train#1700, train loss: 5.559711e-03
train#1710, train loss: 4.926985e-03
train#1720, train loss: 5.303934e-03
train#1730, train loss: 5.521178e-03
train#1740, train loss: 4.670335e-03
train#1750, train loss: 5.631426e-03
train#1760, train loss: 5.041381e-03
train#1770, train loss: 5.511967e-03
train#1780, train loss: 5.419798e-03
train#1790, train loss: 5.468676e-03
train#1800, train loss: 4.732938e-03
train#1810, train loss: 5.094748e-03
train#1820, train loss: 4.670195e-03
train#1830, train loss: 4.773549e-03
train#1840, train loss: 4.837727e-03
train#1850, train loss: 5.216023e-03
train#1860, train loss: 4.190092e-03
train#1870, train loss: 4.832659e-03
train#1880, train loss: 4.834315e-03
train#1890, train loss: 5.145596e-03
train#1900, train loss: 5.440766e-03
train#1910, train loss: 4.799728e-03
train#1920, train loss: 4.486306e-03
train#1930, train loss: 5.332516e-03
train#1940, train loss: 4.697418e-03
train#1950, train loss: 4.898543e-03
train#1960, train loss: 5.023211e-03
train#1970, train loss: 4.910553e-03
train#1980, train loss: 4.454428e-03
train#1990, train loss: 4.239208e-03
train#2000, train loss: 4.461966e-03
losses: [[  1.00000000e+01   5.24970829e-01]
 [  2.00000000e+01   5.02165854e-01]
 [  3.00000000e+01   5.21976292e-01]
 [  4.00000000e+01   5.39231241e-01]
 [  5.00000000e+01   4.54001993e-01]
 [  6.00000000e+01   4.74606544e-01]
 [  7.00000000e+01   5.14866590e-01]
 [  8.00000000e+01   4.63114232e-01]
 [  9.00000000e+01   4.46319312e-01]
 [  1.00000000e+02   5.51044106e-01]
 [  1.10000000e+02   5.19893050e-01]
 [  1.20000000e+02   5.52232981e-01]
 [  1.30000000e+02   5.63150644e-01]
 [  1.40000000e+02   5.12652874e-01]
 [  1.50000000e+02   4.43862528e-01]
 [  1.60000000e+02   4.54757333e-01]
 [  1.70000000e+02   4.07229066e-01]
 [  1.80000000e+02   2.94437885e-01]
 [  1.90000000e+02   1.74438298e-01]
 [  2.00000000e+02   1.17227241e-01]
 [  2.10000000e+02   9.28259119e-02]
 [  2.20000000e+02   8.57229009e-02]
 [  2.30000000e+02   6.66624829e-02]
 [  2.40000000e+02   6.39367551e-02]
 [  2.50000000e+02   5.91579638e-02]
 [  2.60000000e+02   4.92778495e-02]
 [  2.70000000e+02   5.05197495e-02]
 [  2.80000000e+02   4.17561233e-02]
 [  2.90000000e+02   3.70528586e-02]
 [  3.00000000e+02   3.59562337e-02]
 [  3.10000000e+02   3.15468274e-02]
 [  3.20000000e+02   3.34122069e-02]
 [  3.30000000e+02   2.83443686e-02]
 [  3.40000000e+02   2.60360483e-02]
 [  3.50000000e+02   2.84902528e-02]
 [  3.60000000e+02   2.85529494e-02]
 [  3.70000000e+02   2.33652610e-02]
 [  3.80000000e+02   2.33212523e-02]
 [  3.90000000e+02   2.07243487e-02]
 [  4.00000000e+02   2.18675397e-02]
 [  4.10000000e+02   2.17489339e-02]
 [  4.20000000e+02   1.77957509e-02]
 [  4.30000000e+02   1.96765903e-02]
 [  4.40000000e+02   2.10537259e-02]
 [  4.50000000e+02   2.25521345e-02]
 [  4.60000000e+02   2.11623479e-02]
 [  4.70000000e+02   1.88118778e-02]
 [  4.80000000e+02   1.79973412e-02]
 [  4.90000000e+02   1.72890909e-02]
 [  5.00000000e+02   1.77539997e-02]
 [  5.10000000e+02   1.64153595e-02]
 [  5.20000000e+02   1.70677919e-02]
 [  5.30000000e+02   1.59268826e-02]
 [  5.40000000e+02   1.71308592e-02]
 [  5.50000000e+02   1.46934977e-02]
 [  5.60000000e+02   1.35738086e-02]
 [  5.70000000e+02   1.59684848e-02]
 [  5.80000000e+02   1.37811629e-02]
 [  5.90000000e+02   1.39221642e-02]
 [  6.00000000e+02   1.33097507e-02]
 [  6.10000000e+02   1.35420905e-02]
 [  6.20000000e+02   1.25878491e-02]
 [  6.30000000e+02   1.44119551e-02]
 [  6.40000000e+02   1.31819174e-02]
 [  6.50000000e+02   1.35006523e-02]
 [  6.60000000e+02   1.12969885e-02]
 [  6.70000000e+02   1.33185461e-02]
 [  6.80000000e+02   1.28194448e-02]
 [  6.90000000e+02   1.24936672e-02]
 [  7.00000000e+02   1.12930182e-02]
 [  7.10000000e+02   1.07863583e-02]
 [  7.20000000e+02   1.20906942e-02]
 [  7.30000000e+02   1.29156755e-02]
 [  7.40000000e+02   1.05140284e-02]
 [  7.50000000e+02   1.13994172e-02]
 [  7.60000000e+02   1.00526242e-02]
 [  7.70000000e+02   1.07305478e-02]
 [  7.80000000e+02   1.00358725e-02]
 [  7.90000000e+02   1.20549370e-02]
 [  8.00000000e+02   1.20779313e-02]
 [  8.10000000e+02   1.12246787e-02]
 [  8.20000000e+02   1.01809883e-02]
 [  8.30000000e+02   8.49824212e-03]
 [  8.40000000e+02   1.11759268e-02]
 [  8.50000000e+02   9.27789137e-03]
 [  8.60000000e+02   1.04088141e-02]
 [  8.70000000e+02   9.83861461e-03]
 [  8.80000000e+02   9.28709377e-03]
 [  8.90000000e+02   1.02626048e-02]
 [  9.00000000e+02   8.72140378e-03]
 [  9.10000000e+02   8.94727930e-03]
 [  9.20000000e+02   9.43389814e-03]
 [  9.30000000e+02   1.00873765e-02]
 [  9.40000000e+02   8.85191001e-03]
 [  9.50000000e+02   8.38382449e-03]
 [  9.60000000e+02   8.18282925e-03]
 [  9.70000000e+02   9.31667164e-03]
 [  9.80000000e+02   9.57918167e-03]
 [  9.90000000e+02   8.18235893e-03]
 [  1.00000000e+03   9.12148505e-03]
 [  1.01000000e+03   7.68371066e-03]
 [  1.02000000e+03   7.98046310e-03]
 [  1.03000000e+03   7.17705023e-03]
 [  1.04000000e+03   9.03674867e-03]
 [  1.05000000e+03   9.42576490e-03]
 [  1.06000000e+03   7.10476143e-03]
 [  1.07000000e+03   8.26850906e-03]
 [  1.08000000e+03   8.38900916e-03]
 [  1.09000000e+03   8.07069242e-03]
 [  1.10000000e+03   7.83216488e-03]
 [  1.11000000e+03   8.82564113e-03]
 [  1.12000000e+03   6.98517682e-03]
 [  1.13000000e+03   7.92842824e-03]
 [  1.14000000e+03   7.50246877e-03]
 [  1.15000000e+03   7.10098725e-03]
 [  1.16000000e+03   6.98447600e-03]
 [  1.17000000e+03   7.86724500e-03]
 [  1.18000000e+03   7.85678998e-03]
 [  1.19000000e+03   7.57053122e-03]
 [  1.20000000e+03   7.13722548e-03]
 [  1.21000000e+03   6.64981408e-03]
 [  1.22000000e+03   6.94719516e-03]
 [  1.23000000e+03   6.89419266e-03]
 [  1.24000000e+03   6.90645119e-03]
 [  1.25000000e+03   7.21116411e-03]
 [  1.26000000e+03   6.01117266e-03]
 [  1.27000000e+03   7.26602506e-03]
 [  1.28000000e+03   5.97099401e-03]
 [  1.29000000e+03   6.89710630e-03]
 [  1.30000000e+03   6.78111985e-03]
 [  1.31000000e+03   6.99955598e-03]
 [  1.32000000e+03   6.23831525e-03]
 [  1.33000000e+03   6.14073174e-03]
 [  1.34000000e+03   7.05319410e-03]
 [  1.35000000e+03   6.66815741e-03]
 [  1.36000000e+03   5.84342470e-03]
 [  1.37000000e+03   5.81708178e-03]
 [  1.38000000e+03   6.29056944e-03]
 [  1.39000000e+03   6.51696790e-03]
 [  1.40000000e+03   6.52175536e-03]
 [  1.41000000e+03   7.33786216e-03]
 [  1.42000000e+03   6.24056300e-03]
 [  1.43000000e+03   5.86394500e-03]
 [  1.44000000e+03   6.72449730e-03]
 [  1.45000000e+03   6.59021363e-03]
 [  1.46000000e+03   6.42565126e-03]
 [  1.47000000e+03   6.11220906e-03]
 [  1.48000000e+03   5.49821090e-03]
 [  1.49000000e+03   6.15825364e-03]
 [  1.50000000e+03   5.30183455e-03]
 [  1.51000000e+03   5.44701889e-03]
 [  1.52000000e+03   5.96840261e-03]
 [  1.53000000e+03   5.84102143e-03]
 [  1.54000000e+03   6.02063257e-03]
 [  1.55000000e+03   5.79390302e-03]
 [  1.56000000e+03   5.74832549e-03]
 [  1.57000000e+03   5.62326051e-03]
 [  1.58000000e+03   5.37041528e-03]
 [  1.59000000e+03   5.27119264e-03]
 [  1.60000000e+03   6.31239079e-03]
 [  1.61000000e+03   6.53806468e-03]
 [  1.62000000e+03   5.17426711e-03]
 [  1.63000000e+03   5.30378940e-03]
 [  1.64000000e+03   5.91227412e-03]
 [  1.65000000e+03   5.02133137e-03]
 [  1.66000000e+03   4.64896904e-03]
 [  1.67000000e+03   5.38665522e-03]
 [  1.68000000e+03   5.34902327e-03]
 [  1.69000000e+03   5.29049756e-03]
 [  1.70000000e+03   5.55971125e-03]
 [  1.71000000e+03   4.92698513e-03]
 [  1.72000000e+03   5.30393375e-03]
 [  1.73000000e+03   5.52117778e-03]
 [  1.74000000e+03   4.67033545e-03]
 [  1.75000000e+03   5.63142588e-03]
 [  1.76000000e+03   5.04138134e-03]
 [  1.77000000e+03   5.51196700e-03]
 [  1.78000000e+03   5.41979773e-03]
 [  1.79000000e+03   5.46867587e-03]
 [  1.80000000e+03   4.73293802e-03]
 [  1.81000000e+03   5.09474752e-03]
 [  1.82000000e+03   4.67019482e-03]
 [  1.83000000e+03   4.77354927e-03]
 [  1.84000000e+03   4.83772717e-03]
 [  1.85000000e+03   5.21602295e-03]
 [  1.86000000e+03   4.19009151e-03]
 [  1.87000000e+03   4.83265892e-03]
 [  1.88000000e+03   4.83431527e-03]
 [  1.89000000e+03   5.14559634e-03]
 [  1.90000000e+03   5.44076553e-03]
 [  1.91000000e+03   4.79972782e-03]
 [  1.92000000e+03   4.48630610e-03]
 [  1.93000000e+03   5.33251604e-03]
 [  1.94000000e+03   4.69741784e-03]
 [  1.95000000e+03   4.89854254e-03]
 [  1.96000000e+03   5.02321124e-03]
 [  1.97000000e+03   4.91055287e-03]
 [  1.98000000e+03   4.45442786e-03]
 [  1.99000000e+03   4.23920760e-03]
 [  2.00000000e+03   4.46196645e-03]]
initial: [  0.00000000e+00   1.25333234e-01   2.48689887e-01   3.68124553e-01
   4.81753674e-01   5.87785252e-01   6.84547106e-01   7.70513243e-01
   8.44327926e-01   9.04827052e-01   9.51056516e-01   9.82287251e-01
   9.98026728e-01   9.98026728e-01   9.82287251e-01   9.51056516e-01
   9.04827052e-01   8.44327926e-01   7.70513243e-01   6.84547106e-01
   5.87785252e-01   4.81753674e-01   3.68124553e-01   2.48689887e-01
   1.25333234e-01  -3.21624530e-16  -1.25333234e-01  -2.48689887e-01
  -3.68124553e-01  -4.81753674e-01  -5.87785252e-01  -6.84547106e-01
  -7.70513243e-01  -8.44327926e-01  -9.04827052e-01  -9.51056516e-01
  -9.82287251e-01  -9.98026728e-01  -9.98026728e-01  -9.82287251e-01
  -9.51056516e-01  -9.04827052e-01  -8.44327926e-01  -7.70513243e-01
  -6.84547106e-01  -5.87785252e-01  -4.81753674e-01  -3.68124553e-01
  -2.48689887e-01  -1.25333234e-01]
prediction#1, output: -0.086998
prediction#2, output: -0.085649
prediction#3, output: -0.085927
prediction#4, output: -0.086344
prediction#5, output: -0.086798
prediction#6, output: -0.087284
prediction#7, output: -0.087804
prediction#8, output: -0.088359
prediction#9, output: -0.088953
prediction#10, output: -0.089588
prediction#11, output: -0.090267
prediction#12, output: -0.090993
prediction#13, output: -0.091768
prediction#14, output: -0.092597
prediction#15, output: -0.093483
prediction#16, output: -0.094431
prediction#17, output: -0.095443
prediction#18, output: -0.096525
prediction#19, output: -0.097682
prediction#20, output: -0.098918
prediction#21, output: -0.100239
prediction#22, output: -0.101650
prediction#23, output: -0.103159
prediction#24, output: -0.104770
prediction#25, output: -0.106492
prediction#26, output: -0.108332
prediction#27, output: -0.110297
prediction#28, output: -0.112397
prediction#29, output: -0.114639
prediction#30, output: -0.117034
prediction#31, output: -0.119591
prediction#32, output: -0.122322
prediction#33, output: -0.125237
prediction#34, output: -0.128348
prediction#35, output: -0.131669
prediction#36, output: -0.135211
prediction#37, output: -0.138991
prediction#38, output: -0.143022
prediction#39, output: -0.147320
prediction#40, output: -0.151902
prediction#41, output: -0.156784
prediction#42, output: -0.161984
prediction#43, output: -0.167520
prediction#44, output: -0.173412
prediction#45, output: -0.179680
prediction#46, output: -0.186343
prediction#47, output: -0.193421
prediction#48, output: -0.200935
prediction#49, output: -0.208905
prediction#50, output: -0.217352
prediction#51, output: -0.226294
prediction#52, output: -0.235751
prediction#53, output: -0.245738
prediction#54, output: -0.256271
prediction#55, output: -0.267362
prediction#56, output: -0.279019
prediction#57, output: -0.291249
prediction#58, output: -0.304050
prediction#59, output: -0.317418
prediction#60, output: -0.331341
prediction#61, output: -0.345800
prediction#62, output: -0.360768
prediction#63, output: -0.376208
prediction#64, output: -0.392075
prediction#65, output: -0.408315
prediction#66, output: -0.424862
prediction#67, output: -0.441643
prediction#68, output: -0.458577
prediction#69, output: -0.475574
prediction#70, output: -0.492540
prediction#71, output: -0.509377
prediction#72, output: -0.525986
prediction#73, output: -0.542270
prediction#74, output: -0.558137
prediction#75, output: -0.573501
prediction#76, output: -0.588284
prediction#77, output: -0.602422
prediction#78, output: -0.615861
prediction#79, output: -0.628560
prediction#80, output: -0.640493
prediction#81, output: -0.651644
prediction#82, output: -0.662010
prediction#83, output: -0.671602
prediction#84, output: -0.680436
prediction#85, output: -0.688537
prediction#86, output: -0.695938
prediction#87, output: -0.702674
prediction#88, output: -0.708785
prediction#89, output: -0.714312
prediction#90, output: -0.719298
prediction#91, output: -0.723784
prediction#92, output: -0.727812
prediction#93, output: -0.731420
prediction#94, output: -0.734647
prediction#95, output: -0.737528
prediction#96, output: -0.740097
prediction#97, output: -0.742385
prediction#98, output: -0.744419
prediction#99, output: -0.746227
prediction#100, output: -0.747832
outputs: [-0.08699846 -0.08564895 -0.08592695 -0.08634382 -0.08679795 -0.08728403
 -0.08780378 -0.0883593  -0.0889532  -0.08958817 -0.09026688 -0.09099257
 -0.09176815 -0.09259725 -0.09348339 -0.09443063 -0.09544307 -0.09652513
 -0.09768158 -0.09891766 -0.10023862 -0.10165018 -0.10315853 -0.10477018
 -0.10649228 -0.10833204 -0.11029744 -0.11239684 -0.1146391  -0.11703384
 -0.11959106 -0.12232155 -0.12523651 -0.12834799 -0.13166851 -0.13521147
 -0.13899118 -0.14302218 -0.14732033 -0.15190184 -0.1567837  -0.16198361
 -0.16752017 -0.17341238 -0.17967987 -0.18634254 -0.19342071 -0.20093477
 -0.20890528 -0.21735206 -0.22629449 -0.23575097 -0.24573812 -0.256271
 -0.26736164 -0.27901921 -0.29124856 -0.30405    -0.31741825 -0.33134142
 -0.3458004  -0.36076787 -0.37620783 -0.39207509 -0.40831459 -0.42486191
 -0.44164345 -0.45857722 -0.4755742  -0.49253994 -0.5093767  -0.52598572
 -0.54227    -0.55813694 -0.57350063 -0.58828437 -0.60242242 -0.61586124
 -0.62856042 -0.64049268 -0.65164351 -0.66201049 -0.67160195 -0.68043566
 -0.688537   -0.69593763 -0.70267385 -0.708785   -0.71431243 -0.71929824
 -0.72378427 -0.72781163 -0.73141998 -0.73464704 -0.73752838 -0.74009734
 -0.74238485 -0.74441934 -0.74622697 -0.74783158]

real	3m53.845s
user	2m16.754s
sys	2m35.675s
